<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
    <meta charset="utf-8">
    <meta name="generator" content="quarto-1.8.24">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


    <title>蛋白质组学教程</title>
    <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.columns{display: flex; gap: min(4vw, 1.5em);}
      div.column{flex: auto; overflow-x: auto;}
      div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
      ul.task-list{list-style: none;}
      ul.task-list li input[type="checkbox"] {
        width: 0.8em;
        margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
        vertical-align: middle;
      }
      /* CSS for syntax highlighting */
      html { -webkit-text-size-adjust: 100%; }
      pre > code.sourceCode { white-space: pre; position: relative; }
      pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
      pre > code.sourceCode > span:empty { height: 1.2em; }
      .sourceCode { overflow: visible; }
      code.sourceCode > span { color: inherit; text-decoration: inherit; }
      div.sourceCode { margin: 1em 0; }
      pre.sourceCode { margin: 0; }
      @media screen {
      div.sourceCode { overflow: auto; }
      }
      @media print {
      pre > code.sourceCode { white-space: pre-wrap; }
      pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
      }
      pre.numberSource code
        { counter-reset: source-line 0; }
      pre.numberSource code > span
        { position: relative; left: -4em; counter-increment: source-line; }
      pre.numberSource code > span > a:first-child::before
        { content: counter(source-line);
          position: relative; left: -1em; text-align: right; vertical-align: baseline;
          border: none; display: inline-block;
          -webkit-touch-callout: none; -webkit-user-select: none;
          -khtml-user-select: none; -moz-user-select: none;
          -ms-user-select: none; user-select: none;
          padding: 0 4px; width: 4em;
        }
      pre.numberSource { margin-left: 3em;  padding-left: 4px; }
      div.sourceCode
        {   }
      @media screen {
      pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
      }
    </style>

    <style>
      body.hypothesis-enabled #quarto-embed-header {
        padding-right: 36px;
      }

      #quarto-embed-header {
        height: 3em;
        width: 100%;
        display: flex;
        justify-content: space-between;
        align-items: center;
        border-bottom: solid 1px;
      }

      #quarto-embed-header h6 {
        font-size: 1.1em;
        padding-top: 0.6em;
        margin-left: 1em;
        margin-right: 1em;
        font-weight: 400;
      }

      #quarto-embed-header a.quarto-back-link,
      #quarto-embed-header a.quarto-download-embed {
        font-size: 0.8em;
        margin-top: 1em;
        margin-bottom: 1em;
        margin-left: 1em;
        margin-right: 1em;
      }

      .quarto-back-container {
        padding-left: 0.5em;
        display: flex;
      }

      .headroom {
          will-change: transform;
          transition: transform 200ms linear;
      }

      .headroom--pinned {
          transform: translateY(0%);
      }

      .headroom--unpinned {
          transform: translateY(-100%);
      }      
    </style>

    <script>
    window.document.addEventListener("DOMContentLoaded", function () {

      var header = window.document.querySelector("#quarto-embed-header");
      const titleBannerEl = window.document.querySelector("body > #title-block-header");
      if (titleBannerEl) {
        titleBannerEl.style.paddingTop = header.clientHeight + "px";
      }
      const contentEl = window.document.getElementById('quarto-content');
      for (const child of contentEl.children) {
        child.style.paddingTop = header.clientHeight + "px";
        child.style.marginTop = "1em";
      }

      // Use the article root if the `back` call doesn't work. This isn't perfect
      // but should typically work
      window.quartoBackToArticle = () => {
        var currentUrl = window.location.href;
        window.history.back();
        setTimeout(() => {
            // if location was not changed in 100 ms, then there is no history back
            if(currentUrl === window.location.href){              
                // redirect to site root
                window.location.href = "index.html";
            }
        }, 100);
      }

      const headroom = new window.Headroom(header, {
        tolerance: 5,
        onPin: function () {
        },
        onUnpin: function () {
        },
      });
      headroom.init();
    });
    </script>

    
<script src="site_libs/manuscript-notebook/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-4d7f0bce1131f3e5f9547cd857cfbfc8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
     <script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>  
      </head>

  <body class="quarto-notebook quarto-light">
    <div id="quarto-embed-header" class="headroom fixed-top bg-primary">
      
      <a onclick="window.quartoBackToArticle(); return false;" class="btn btn-primary quarto-back-link" href=""><i class="bi bi-caret-left"></i> Back to Article</a>
      <h6><i class="bi bi-journal-code"></i> 蛋白质组学教程</h6>

            <a href="./proteomic.qmd" class="btn btn-primary quarto-download-embed" download="proteomic.qmd">Download Source</a>
          </div>

     <header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">蛋白质组学教程</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">JAYZ </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        The University OF Myself
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>



    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#proteomic" id="toc-proteomic" class="nav-link active" data-scroll-target="#proteomic">😉Proteomic❤️</a>
  <ul class="collapse">
  <li><a href="#understanding-the-raw-data-format" id="toc-understanding-the-raw-data-format" class="nav-link" data-scroll-target="#understanding-the-raw-data-format">Understanding the Raw Data Format😎</a>
  <ul class="collapse">
  <li><a href="#proprietary-vs.-open-standard-formats" id="toc-proprietary-vs.-open-standard-formats" class="nav-link" data-scroll-target="#proprietary-vs.-open-standard-formats">Proprietary vs.&nbsp;Open-Standard Formats</a></li>
  <li><a href="#the-key-information-inside-the-file-a-scan-by-scan-record" id="toc-the-key-information-inside-the-file-a-scan-by-scan-record" class="nav-link" data-scroll-target="#the-key-information-inside-the-file-a-scan-by-scan-record">The Key Information Inside the File: A Scan-by-Scan Record</a></li>
  <li><a href="#summary-for-this-lesson" id="toc-summary-for-this-lesson" class="nav-link" data-scroll-target="#summary-for-this-lesson">Summary for this Lesson</a></li>
  </ul></li>
  <li><a href="#quality-control-on-the-raw-data" id="toc-quality-control-on-the-raw-data" class="nav-link" data-scroll-target="#quality-control-on-the-raw-data">Quality Control on the Raw Data👺</a>
  <ul class="collapse">
  <li><a href="#metric-1-the-total-ion-chromatogram-tic" id="toc-metric-1-the-total-ion-chromatogram-tic" class="nav-link" data-scroll-target="#metric-1-the-total-ion-chromatogram-tic">Metric 1: The Total Ion Chromatogram (TIC)</a></li>
  <li><a href="#metric-2-mass-accuracy" id="toc-metric-2-mass-accuracy" class="nav-link" data-scroll-target="#metric-2-mass-accuracy">Metric 2: Mass Accuracy</a></li>
  <li><a href="#metric-3-consistency-between-replicates" id="toc-metric-3-consistency-between-replicates" class="nav-link" data-scroll-target="#metric-3-consistency-between-replicates">Metric 3: Consistency Between Replicates</a></li>
  </ul></li>
  <li><a href="#spectral-processing---cleaning-the-signal" id="toc-spectral-processing---cleaning-the-signal" class="nav-link" data-scroll-target="#spectral-processing---cleaning-the-signal">Spectral Processing - Cleaning the Signal🤯</a>
  <ul class="collapse">
  <li><a href="#step-1-peak-picking-or-centroiding质心" id="toc-step-1-peak-picking-or-centroiding质心" class="nav-link" data-scroll-target="#step-1-peak-picking-or-centroiding质心">Step 1: Peak Picking (or “Centroiding”(质心))</a></li>
  <li><a href="#step-2-de-isotoping去同位素" id="toc-step-2-de-isotoping去同位素" class="nav-link" data-scroll-target="#step-2-de-isotoping去同位素">Step 2: De-isotoping(去同位素)</a></li>
  <li><a href="#step-3-noise-reduction" id="toc-step-3-noise-reduction" class="nav-link" data-scroll-target="#step-3-noise-reduction">Step 3: Noise Reduction</a></li>
  </ul></li>
  <li><a href="#the-search-engine---the-matching-game." id="toc-the-search-engine---the-matching-game." class="nav-link" data-scroll-target="#the-search-engine---the-matching-game.">The Search Engine - The Matching Game.👻</a>
  <ul class="collapse">
  <li><a href="#step-1-get-the-list-of-suspects-the-protein-database" id="toc-step-1-get-the-list-of-suspects-the-protein-database" class="nav-link" data-scroll-target="#step-1-get-the-list-of-suspects-the-protein-database">Step 1: Get the List of Suspects (The Protein Database)</a></li>
  <li><a href="#step-2-create-a-master-list-of-all-possible-peptides-in-silico-digestion" id="toc-step-2-create-a-master-list-of-all-possible-peptides-in-silico-digestion" class="nav-link" data-scroll-target="#step-2-create-a-master-list-of-all-possible-peptides-in-silico-digestion">Step 2: Create a Master List of All Possible Peptides (In Silico Digestion)</a></li>
  <li><a href="#step-3-predict-the-fingerprint-for-each-possible-peptide-theoretical-spectrum-generation" id="toc-step-3-predict-the-fingerprint-for-each-possible-peptide-theoretical-spectrum-generation" class="nav-link" data-scroll-target="#step-3-predict-the-fingerprint-for-each-possible-peptide-theoretical-spectrum-generation">Step 3: Predict the Fingerprint for Each Possible Peptide (Theoretical Spectrum Generation)</a></li>
  <li><a href="#step-4-the-match-and-score" id="toc-step-4-the-match-and-score" class="nav-link" data-scroll-target="#step-4-the-match-and-score">Step 4: The Match and Score</a></li>
  <li><a href="#the-final-output-the-peptide-spectrum-match-psm" id="toc-the-final-output-the-peptide-spectrum-match-psm" class="nav-link" data-scroll-target="#the-final-output-the-peptide-spectrum-match-psm">The Final Output: The Peptide-Spectrum Match (PSM)</a></li>
  <li><a href="#the-problem-the-inevitability-of-false-positives" id="toc-the-problem-the-inevitability-of-false-positives" class="nav-link" data-scroll-target="#the-problem-the-inevitability-of-false-positives">The Problem: The Inevitability of False Positives</a></li>
  <li><a href="#the-solution-the-target-decoy-strategy" id="toc-the-solution-the-target-decoy-strategy" class="nav-link" data-scroll-target="#the-solution-the-target-decoy-strategy">The Solution: The Target-Decoy Strategy</a></li>
  <li><a href="#summary-for-this-lesson-1" id="toc-summary-for-this-lesson-1" class="nav-link" data-scroll-target="#summary-for-this-lesson-1">Summary for this Lesson</a></li>
  </ul></li>
  <li><a href="#the-protein-inference-problem---from-peptides-to-proteins" id="toc-the-protein-inference-problem---from-peptides-to-proteins" class="nav-link" data-scroll-target="#the-protein-inference-problem---from-peptides-to-proteins">The Protein inference Problem - from peptides to proteins 👹</a>
  <ul class="collapse">
  <li><a href="#goal-understand-the-challenge-of-accurately-identifying-proteins-from-a-list-of-peptides-and-the-logic-used-to-solve-it." id="toc-goal-understand-the-challenge-of-accurately-identifying-proteins-from-a-list-of-peptides-and-the-logic-used-to-solve-it." class="nav-link" data-scroll-target="#goal-understand-the-challenge-of-accurately-identifying-proteins-from-a-list-of-peptides-and-the-logic-used-to-solve-it.">Goal: Understand the challenge of accurately identifying proteins from a list of peptides and the logic used to solve it.</a></li>
  <li><a href="#the-core-problem-ambiguity歧义-from-shared-peptides" id="toc-the-core-problem-ambiguity歧义-from-shared-peptides" class="nav-link" data-scroll-target="#the-core-problem-ambiguity歧义-from-shared-peptides">The Core Problem: Ambiguity(歧义) from Shared Peptides</a></li>
  <li><a href="#the-solution-the-principle-of-parsimony简约-occams-razor奥卡姆剃刀" id="toc-the-solution-the-principle-of-parsimony简约-occams-razor奥卡姆剃刀" class="nav-link" data-scroll-target="#the-solution-the-principle-of-parsimony简约-occams-razor奥卡姆剃刀">The Solution: The Principle of Parsimony(简约) (Occam’s Razor(奥卡姆剃刀))</a></li>
  <li><a href="#the-output-protein-groups" id="toc-the-output-protein-groups" class="nav-link" data-scroll-target="#the-output-protein-groups">The Output: Protein Groups</a></li>
  <li><a href="#summary-for-this-lesson-2" id="toc-summary-for-this-lesson-2" class="nav-link" data-scroll-target="#summary-for-this-lesson-2">Summary for this Lesson</a></li>
  </ul></li>
  <li><a href="#quantification" id="toc-quantification" class="nav-link" data-scroll-target="#quantification">Quantification😋</a>
  <ul class="collapse">
  <li><a href="#goal-learn-the-computational-methods-for-determining-the-abundance-of-proteins." id="toc-goal-learn-the-computational-methods-for-determining-the-abundance-of-proteins." class="nav-link" data-scroll-target="#goal-learn-the-computational-methods-for-determining-the-abundance-of-proteins.">Goal: Learn the computational methods for determining the abundance of proteins.</a></li>
  <li><a href="#method-1-label-free-quantification-lfq无定量标记" id="toc-method-1-label-free-quantification-lfq无定量标记" class="nav-link" data-scroll-target="#method-1-label-free-quantification-lfq无定量标记">Method 1: Label-Free Quantification (LFQ)(无定量标记)</a></li>
  <li><a href="#method-2-isobaric-labeling-tmt-and-itraq同量异位素标记" id="toc-method-2-isobaric-labeling-tmt-and-itraq同量异位素标记" class="nav-link" data-scroll-target="#method-2-isobaric-labeling-tmt-and-itraq同量异位素标记">Method 2: Isobaric Labeling (TMT and iTRAQ)(同量异位素标记)</a></li>
  <li><a href="#final-critical-step-for-both-methods-normalization" id="toc-final-critical-step-for-both-methods-normalization" class="nav-link" data-scroll-target="#final-critical-step-for-both-methods-normalization">Final Critical Step for Both Methods: Normalization</a></li>
  <li><a href="#summary-for-this-lesson-3" id="toc-summary-for-this-lesson-3" class="nav-link" data-scroll-target="#summary-for-this-lesson-3">Summary for this Lesson</a></li>
  </ul></li>
  <li><a href="#downstream-analysis" id="toc-downstream-analysis" class="nav-link" data-scroll-target="#downstream-analysis">🐗DownStream analysis🐻</a></li>
  <li><a href="#downstream-statistics" id="toc-downstream-statistics" class="nav-link" data-scroll-target="#downstream-statistics">Downstream Statistics🙂‍↔︎️</a>
  <ul class="collapse">
  <li><a href="#goal-learn-how-to-find-statistically-significant-changes-between-your-sample-groups-from-your-quantified-protein-list." id="toc-goal-learn-how-to-find-statistically-significant-changes-between-your-sample-groups-from-your-quantified-protein-list." class="nav-link" data-scroll-target="#goal-learn-how-to-find-statistically-significant-changes-between-your-sample-groups-from-your-quantified-protein-list.">Goal: Learn how to find statistically significant changes between your sample groups from your quantified protein list.</a></li>
  <li><a href="#step-1-handling-missing-values-data-imputation" id="toc-step-1-handling-missing-values-data-imputation" class="nav-link" data-scroll-target="#step-1-handling-missing-values-data-imputation">Step 1: Handling Missing Values (Data Imputation)</a></li>
  <li><a href="#step-2-log-transformation" id="toc-step-2-log-transformation" class="nav-link" data-scroll-target="#step-2-log-transformation">Step 2: Log Transformation</a></li>
  <li><a href="#step-3-performing-the-statistical-test" id="toc-step-3-performing-the-statistical-test" class="nav-link" data-scroll-target="#step-3-performing-the-statistical-test">Step 3: Performing the Statistical Test</a></li>
  <li><a href="#step-4-the-multiple-hypothesis-testing-problem-and-correction" id="toc-step-4-the-multiple-hypothesis-testing-problem-and-correction" class="nav-link" data-scroll-target="#step-4-the-multiple-hypothesis-testing-problem-and-correction">Step 4: The Multiple Hypothesis Testing Problem and Correction</a></li>
  <li><a href="#step-5-visualizing-the-results-the-volcano-plot" id="toc-step-5-visualizing-the-results-the-volcano-plot" class="nav-link" data-scroll-target="#step-5-visualizing-the-results-the-volcano-plot">Step 5: Visualizing the Results: The Volcano Plot</a></li>
  <li><a href="#summary-for-this-lesson-4" id="toc-summary-for-this-lesson-4" class="nav-link" data-scroll-target="#summary-for-this-lesson-4">Summary for this Lesson</a></li>
  </ul></li>
  <li><a href="#go-and-kegg" id="toc-go-and-kegg" class="nav-link" data-scroll-target="#go-and-kegg">Go and KEGG 😏</a>
  <ul class="collapse">
  <li><a href="#goal-go-from-a-list-of-significant-proteins-to-a-coherent-biological-story." id="toc-goal-go-from-a-list-of-significant-proteins-to-a-coherent-biological-story." class="nav-link" data-scroll-target="#goal-go-from-a-list-of-significant-proteins-to-a-coherent-biological-story.">Goal: Go from a list of significant proteins to a coherent biological story.</a></li>
  <li><a href="#the-tools-proteomics-knowledge-bases" id="toc-the-tools-proteomics-knowledge-bases" class="nav-link" data-scroll-target="#the-tools-proteomics-knowledge-bases">The Tools: Proteomics Knowledge Bases</a></li>
  <li><a href="#the-method-the-statistics-of-over-representation" id="toc-the-method-the-statistics-of-over-representation" class="nav-link" data-scroll-target="#the-method-the-statistics-of-over-representation">The Method: The Statistics of “Over-representation”</a></li>
  <li><a href="#the-workflow-and-output" id="toc-the-workflow-and-output" class="nav-link" data-scroll-target="#the-workflow-and-output">The Workflow and Output</a></li>
  <li><a href="#summary-for-this-lesson-5" id="toc-summary-for-this-lesson-5" class="nav-link" data-scroll-target="#summary-for-this-lesson-5">Summary for this Lesson</a></li>
  </ul></li>
  <li><a href="#network-analysis" id="toc-network-analysis" class="nav-link" data-scroll-target="#network-analysis">Network Analysis🥶</a>
  <ul class="collapse">
  <li><a href="#goal-visualize-and-analyze-how-the-changing-proteins-interact-with-each-other-to-form-a-functional-system." id="toc-goal-visualize-and-analyze-how-the-changing-proteins-interact-with-each-other-to-form-a-functional-system." class="nav-link" data-scroll-target="#goal-visualize-and-analyze-how-the-changing-proteins-interact-with-each-other-to-form-a-functional-system.">Goal: Visualize and analyze how the changing proteins interact with each other to form a functional system.</a></li>
  <li><a href="#the-tool-protein-protein-interaction-ppi-databases" id="toc-the-tool-protein-protein-interaction-ppi-databases" class="nav-link" data-scroll-target="#the-tool-protein-protein-interaction-ppi-databases">The Tool: Protein-Protein Interaction (PPI) Databases</a></li>
  <li><a href="#the-workflow-building-and-interpreting-a-network" id="toc-the-workflow-building-and-interpreting-a-network" class="nav-link" data-scroll-target="#the-workflow-building-and-interpreting-a-network">The Workflow: Building and Interpreting a Network</a></li>
  <li><a href="#example-interpretation" id="toc-example-interpretation" class="nav-link" data-scroll-target="#example-interpretation">Example Interpretation</a></li>
  <li><a href="#summary-for-this-lesson-6" id="toc-summary-for-this-lesson-6" class="nav-link" data-scroll-target="#summary-for-this-lesson-6">Summary for this Lesson</a></li>
  <li><a href="#course-conclusion" id="toc-course-conclusion" class="nav-link" data-scroll-target="#course-conclusion">Course Conclusion</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#ture-project" id="toc-ture-project" class="nav-link" data-scroll-target="#ture-project">🥺TURE Project🦄</a>
  <ul class="collapse">
  <li><a href="#project-the-effect-of-a-kinase-inhibitor激酶抑制剂-on-the-hela-cell-proteome" id="toc-project-the-effect-of-a-kinase-inhibitor激酶抑制剂-on-the-hela-cell-proteome" class="nav-link" data-scroll-target="#project-the-effect-of-a-kinase-inhibitor激酶抑制剂-on-the-hela-cell-proteome"><strong>Project: The Effect of a Kinase Inhibitor(激酶抑制剂) on the HeLa Cell Proteome</strong></a></li>
  <li><a href="#part-0-project-setup-data-acquisition-and-organization" id="toc-part-0-project-setup-data-acquisition-and-organization" class="nav-link" data-scroll-target="#part-0-project-setup-data-acquisition-and-organization"><strong>Part 0: Project Setup, Data Acquisition, and Organization</strong></a></li>
  <li><a href="#part-1-r-environment-setup-data-conversion-and-project-organization" id="toc-part-1-r-environment-setup-data-conversion-and-project-organization" class="nav-link" data-scroll-target="#part-1-r-environment-setup-data-conversion-and-project-organization"><strong>Part 1: R Environment Setup, Data Conversion, and Project Organization</strong></a></li>
  <li><a href="#end-of-part-1-status-check" id="toc-end-of-part-1-status-check" class="nav-link" data-scroll-target="#end-of-part-1-status-check"><strong>End of Part 1: Status Check</strong></a></li>
  <li><a href="#part-2-data-loading-and-initial-quality-control-with-r" id="toc-part-2-data-loading-and-initial-quality-control-with-r" class="nav-link" data-scroll-target="#part-2-data-loading-and-initial-quality-control-with-r"><strong>Part 2: Data Loading and Initial Quality Control with R</strong></a></li>
  <li><a href="#end-of-part-2-status-check" id="toc-end-of-part-2-status-check" class="nav-link" data-scroll-target="#end-of-part-2-status-check"><strong>End of Part 2: Status Check</strong></a></li>
  <li><a href="#part-3-peptide-identification-using-a-standalone-search-engine独立搜索引擎" id="toc-part-3-peptide-identification-using-a-standalone-search-engine独立搜索引擎" class="nav-link" data-scroll-target="#part-3-peptide-identification-using-a-standalone-search-engine独立搜索引擎"><strong>Part 3: Peptide Identification using a Standalone Search Engine(独立搜索引擎)</strong></a></li>
  <li><a href="#end-of-part-3-status-check" id="toc-end-of-part-3-status-check" class="nav-link" data-scroll-target="#end-of-part-3-status-check"><strong>End of Part 3: Status Check</strong></a></li>
  <li><a href="#part-4-importing-identifications-and-performing-ms1-level-quantification-in-r" id="toc-part-4-importing-identifications-and-performing-ms1-level-quantification-in-r" class="nav-link" data-scroll-target="#part-4-importing-identifications-and-performing-ms1-level-quantification-in-r"><strong>Part 4: Importing Identifications and Performing MS1-Level Quantification in R</strong></a></li>
  <li><a href="#end-of-part-4-status-check" id="toc-end-of-part-4-status-check" class="nav-link" data-scroll-target="#end-of-part-4-status-check"><strong>End of Part 4: Status Check</strong></a></li>
  <li><a href="#filter-the-protein" id="toc-filter-the-protein" class="nav-link" data-scroll-target="#filter-the-protein">Filter the Protein</a></li>
  <li><a href="#the-problem-with-using-a-pre-filtered-protein-list-for-quantification" id="toc-the-problem-with-using-a-pre-filtered-protein-list-for-quantification" class="nav-link" data-scroll-target="#the-problem-with-using-a-pre-filtered-protein-list-for-quantification">The Problem with Using a Pre-Filtered Protein List for Quantification</a></li>
  <li><a href="#the-smarter-approach-used-by-alfq-and-maxquant" id="toc-the-smarter-approach-used-by-alfq-and-maxquant" class="nav-link" data-scroll-target="#the-smarter-approach-used-by-alfq-and-maxquant">The Smarter Approach Used by aLFQ (and MaxQuant)</a></li>
  <li><a href="#part-5-differential-expression-analysis-and-biological-interpretation" id="toc-part-5-differential-expression-analysis-and-biological-interpretation" class="nav-link" data-scroll-target="#part-5-differential-expression-analysis-and-biological-interpretation"><strong>Part 5: Differential Expression Analysis and Biological Interpretation</strong></a></li>
  <li><a href="#part-6-biological-interpretation---uncovering-the-story" id="toc-part-6-biological-interpretation---uncovering-the-story" class="nav-link" data-scroll-target="#part-6-biological-interpretation---uncovering-the-story"><strong>Part 6: Biological Interpretation - Uncovering the Story</strong></a></li>
  <li><a href="#grand-conclusion-from-raw-data-to-a-biological-hypothesis" id="toc-grand-conclusion-from-raw-data-to-a-biological-hypothesis" class="nav-link" data-scroll-target="#grand-conclusion-from-raw-data-to-a-biological-hypothesis"><strong>Grand Conclusion: From Raw Data to a Biological Hypothesis</strong></a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">      

       <section id="proteomic" class="level1">
<h1>😉Proteomic❤️</h1>
<hr>
<section id="understanding-the-raw-data-format" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-raw-data-format">Understanding the Raw Data Format😎</h2>
<section id="proprietary-vs.-open-standard-formats" class="level3">
<h3 class="anchored" data-anchor-id="proprietary-vs.-open-standard-formats">Proprietary vs.&nbsp;Open-Standard Formats</h3>
<p>When the mass spectrometer finishes its run, <strong>it saves the data in a specific file format</strong>. Initially, these are almost always in a proprietary format9专有格式), meaning it’s specific to the vendor(制造商) who made the instrument.</p>
<ul>
<li><p><strong>Proprietary Formats:</strong></p>
<ul>
<li><p>Thermo Scientific: <strong><code>.raw</code></strong></p></li>
<li><p>Sciex: <strong>.<code>wiff</code></strong></p></li>
<li><p>Agilent: <strong>.<code>d</code></strong></p></li>
<li><p>Bruker: <strong><code>.baf</code></strong></p></li>
<li><p><strong>Problem:</strong> You often <strong>need the vendor’s specific software</strong> to open and read these files. This is not ideal for bioinformaticians who want to use open-source tools and write their own scripts.</p></li>
</ul></li>
<li><p><strong>The Open-Standard: <em>mzML</em></strong></p>
<ul>
<li><p>To solve this problem, the proteomics community developed a standardized, open format called <strong>mzML</strong>.</p></li>
<li><p><strong>Analogy:</strong> Think of .raw as a Photoshop file (.psd) and mzML as a JPEG (.jpg). The JPEG is a universal format that any program can open.</p></li>
<li><p><strong>Key Tool:</strong> A program called <strong>ProteoWizard</strong> (specifically its msConvert tool) is the universally accepted “translator”. As a bioinformatician, one of your first steps is often to convert all the proprietary .raw or .wiff files into the open mzML format. This allows you to use a wide variety of analysis software.</p></li>
</ul></li>
</ul>
</section>
<section id="the-key-information-inside-the-file-a-scan-by-scan-record" class="level3">
<h3 class="anchored" data-anchor-id="the-key-information-inside-the-file-a-scan-by-scan-record">The Key Information Inside the File: A Scan-by-Scan Record</h3>
<p>Whether it’s a .raw or mzML file, the data is organized as a long series of <strong>“scans.”</strong> Each scan is a snapshot of the ions entering the mass spectrometer at a specific point in time.</p>
<p>Here are the essential pieces of information recorded for every single scan.</p>
<p><strong>1. Scan Number(扫描数量):</strong></p>
<ul>
<li><p><strong>What it is:</strong> A simple, unique counter for each scan (e.g., Scan 1, Scan 2, … Scan 50,000).</p></li>
<li><p><strong>Why it matters:</strong> It’s the primary identifier for a specific spectrum within the file.</p></li>
</ul>
<p><strong>2. Retention Time(保留时间) (RT):</strong></p>
<ul>
<li><p><strong>What it is:</strong> The time (usually in minutes) when the scan was taken. It corresponds to the time the peptide eluted(洗脱), or came off, the liquid chromatography (LC) column(液相色谱).</p></li>
<li><p><strong>Why it matters:</strong> It helps separate peptides in time before they enter the mass spectrometer. Peptides with different chemical properties will have different retention times.</p></li>
</ul>
<p><strong>3. MS Level (or Scan Type):</strong><br>
This is the most critical piece of information for understanding what the machine was doing at that moment.</p>
<ul>
<li><p><strong>MS1 Scan (also called a “Survey Scan”):</strong></p>
<ul>
<li><p><strong>What it is:</strong> A snapshot of all the peptide ions entering the mass spec at a given retention time. It’s a map of mass-to-charge (m/z) versus intensity(质荷比和强度的图谱).</p></li>
<li><p><strong>Purpose:</strong> To identify the “parent”(母离子) or “precursor”(前体离子) ions that are present and to measure their abundance (intensity). This data is primarily used for <em><strong>quantification</strong>.</em></p></li>
</ul></li>
<li><p><strong>MS2 Scan (also called MS/MS or “Fragmentation Scan”):</strong></p>
<ul>
<li><p><strong>What it is:</strong> In the preceding MS1 scan, the instrument <strong><em>picked one of the most intense precursor ions,</em></strong> isolated it, broke it into fragments, and then took a picture of those fragments.</p></li>
<li><p><strong>Purpose:</strong> <strong>To identify the amino acid sequence of the peptide</strong>. The fragmentation pattern is like a fingerprint unique to that peptide. This data is primarily used for <em><strong>identification</strong>.</em></p></li>
</ul></li>
</ul>
<p><strong>4. Precursor m/z (Mass-to-Charge Ratio（质荷比)):</strong></p>
<ul>
<li><p><strong>What it is:</strong> This only applies to MS2 scans. It is the m/z of the parent ion that was selected from the preceding MS1 scan to be fragmented.</p></li>
<li><p><strong>Why it matters:</strong> It’s the <strong>crucial link between an MS2 spectrum (the fragments) and the MS1 parent ion</strong> it came from. The database search engine will use this mass very precisely to narrow down which peptides it could possibly be.</p></li>
</ul>
<p><strong>5. Ion Intensities and m/z Arrays:</strong></p>
<ul>
<li><p><strong>What it is:</strong> For every scan (MS1 or MS2), the core data is a list of two numbers: a mass-to-charge (m/z) value and its corresponding intensity (a measure of how many ions were detected at that m/z).</p></li>
<li><p><strong>Why it matters:</strong> This is the actual spectrum!</p>
<ul>
<li><p>In an <strong>MS1 scan</strong>, the intensity of a precursor ion is used to <strong>quantify</strong> how much of that peptide was in the sample.</p></li>
<li><p>In an <strong>MS2 scan</strong>, the m/z and intensity of the fragment ions are used to create the “fingerprint” for <strong>database searching.</strong></p></li>
</ul></li>
</ul>
</section>
<section id="summary-for-this-lesson" class="level3">
<h3 class="anchored" data-anchor-id="summary-for-this-lesson">Summary for this Lesson</h3>
<p>As a bioinformatician, when you receive a raw data file, you should immediately think of it as a structured collection of scans. Your job is to use software to extract this information. The primary goal of the next analysis steps will be to:</p>
<ol type="1">
<li><p>Use the <strong>MS1 scans</strong> to find all the peptide precursors and measure their <strong>intensity</strong> at a specific <strong>retention time</strong>.</p></li>
<li><p>Link each <strong>MS2 scan</strong> back to its precursor using the <strong>precursor m/z</strong>.</p></li>
<li><p>Use the fragment ion data within the <strong>MS2 scans</strong> to figure out the peptide’s identity.</p></li>
</ol>
<p>This fundamental structure—alternating MS1 survey scans and MS2 fragmentation scans—is the basis of nearly all modern proteomics experiments.</p>
<hr>
</section>
</section>
<section id="quality-control-on-the-raw-data" class="level2">
<h2 class="anchored" data-anchor-id="quality-control-on-the-raw-data">Quality Control on the Raw Data👺</h2>
<p><strong><em>Most proteomics analysis software (like MaxQuant or vendor-specific suites) will generate a QC report for you, but you need to know what to look for. Here are the key metrics you will check.</em></strong></p>
<section id="metric-1-the-total-ion-chromatogram-tic" class="level3">
<h3 class="anchored" data-anchor-id="metric-1-the-total-ion-chromatogram-tic">Metric 1: The Total Ion Chromatogram (TIC)</h3>
<ul>
<li><p><strong>What it is:</strong> The TIC is the sum of all the ion intensities in every <strong>MS1 scan</strong>, plotted against the retention time. It gives you a bird’s-eye view of the entire run.</p></li>
<li><p><strong>What you’re looking at:</strong> The overall stability and intensity of the signal over time.</p></li>
<li><p><strong>What a GOOD TIC looks like:</strong></p>
<ul>
<li><p>It often has a relatively smooth, somewhat rectangular(略显矩形) or broad bell shape.</p></li>
<li><p>The intensity rises as the peptides start eluting from the LC column, stays high and stable during the main part of the gradient, and then drops off as the run finishes.</p></li>
<li><p>Think of it like a stable heartbeat for the experiment. It shows the electrospray(电子喷雾) was consistent and peptides were eluting steadily.</p></li>
</ul></li>
<li><p><strong>What a BAD TIC looks like:</strong></p>
<ul>
<li><p><strong>Extremely spiky or jagged(锯齿状):</strong> This can indicate an unstable electrospray.</p></li>
<li><p><strong>Sudden, sharp drops to zero intensity:</strong> This could mean the spray failed entirely for a period of time. No data was collected in that window.</p></li>
<li><p><strong>Very low intensity overall:</strong> The sample might have been too diluted.</p></li>
</ul></li>
<li><p><strong>Why it matters:</strong> If the TIC is unstable, the quantification data derived from the MS1 scans will be unreliable. A run with a failed spray is often unusable and should be removed from the analysis.</p></li>
</ul>
</section>
<section id="metric-2-mass-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="metric-2-mass-accuracy">Metric 2: Mass Accuracy</h3>
<ul>
<li><p><strong>What it is:</strong> The difference between the experimentally measured m/z of a peptide and its theoretically calculated true m/z. This is measured in <strong>parts-per-million (ppm).</strong></p></li>
<li><p><code>ppm = [(Measured Mass - True Mass) / True Mass] * 1,000,000.</code></p></li>
<li><p><strong>What you’re looking at:</strong> How accurate and stable the instrument’s mass measurement was during the run.</p></li>
<li><p><strong>What GOOD Mass Accuracy looks like:</strong></p>
<ul>
<li><p>A very tight distribution of mass errors centered close to zero.</p></li>
<li><p>For a modern Orbitrap instrument, the vast majority of identified peptides should have a mass error of <strong>less than 5 ppm</strong>, and often less than 2 ppm.</p></li>
<li><p>The plot of mass error over retention time should be flat, not drifting up or down.</p></li>
</ul></li>
<li><p><strong>What BAD Mass Accuracy looks like:</strong></p>
<ul>
<li><p>A very wide distribution of errors (e.g., &gt; 10-20 ppm).</p></li>
<li><p>A systematic drift where the error is, for example, -10 ppm at the start of the run and +10 ppm at the end.</p></li>
<li><p>The center of the distribution is far from zero (e.g., centered on 15 ppm).</p></li>
</ul></li>
<li><p><strong>Why it matters:</strong> High mass accuracy is your most powerful tool for identifying peptides correctly. When you search the database, you’ll tell the search engine to only consider theoretical peptides within a narrow mass window (e.g., 10 ppm) of what you measured. If the mass accuracy is poor, you either have to use a very wide window (which dramatically increases the chance of false positives) or you will fail to identify many peptides. Poor mass accuracy often means the instrument needs to be re-calibrated(重新校准).</p></li>
</ul>
</section>
<section id="metric-3-consistency-between-replicates" class="level3">
<h3 class="anchored" data-anchor-id="metric-3-consistency-between-replicates">Metric 3: Consistency Between Replicates</h3>
<p>Science demands reproducibility. In proteomics, you will almost always analyze multiple runs of the same sample (technical replicates) or of different samples from the same group (biological replicates).</p>
<ul>
<li><p><strong>What you’re looking at:</strong> You compare the QC metrics above across all the related files in your experiment.</p></li>
<li><p><strong>What GOOD Consistency looks like:</strong></p>
<ul>
<li><p>The TIC shapes and intensity ranges are very similar across replicate runs.</p></li>
<li><p>After a preliminary analysis, the total number of identified proteins and peptides is in the same ballpark for each replicate.</p></li>
</ul></li>
<li><p><strong>What BAD Consistency looks like:</strong></p>
<ul>
<li>One run (an “outlier”) has a collapsed TIC, much lower intensity, or identifies half as many proteins as its partners.</li>
</ul></li>
<li><p><strong>Why it matters:</strong> If one replicate is a clear technical failure, it’s often better to <strong>exclude it from the analysis</strong>. Including a bad outlier run can severely compromise your downstream statistical analysis and lead to incorrect conclusions.</p></li>
</ul>
<hr>
</section>
</section>
<section id="spectral-processing---cleaning-the-signal" class="level2">
<h2 class="anchored" data-anchor-id="spectral-processing---cleaning-the-signal">Spectral Processing - Cleaning the Signal🤯</h2>
<p><strong><em>Imagine the raw spectrum from the mass spectrometer is a blurry, noisy photograph. You can see the general shapes, but it’s hard to make out the important details. Spectral processing is the equivalent of using photo editing software to sharpen the image, increase the contrast, and remove the background noise, so you are left with a clear picture of the main subject.</em></strong></p>
<section id="step-1-peak-picking-or-centroiding质心" class="level3">
<h3 class="anchored" data-anchor-id="step-1-peak-picking-or-centroiding质心">Step 1: Peak Picking (or “Centroiding”(质心))</h3>
<p>This is the most fundamental step of spectral processing. The raw output from the detector is not a list of perfect peaks, but a continuous “profile”(轮廓) of ion intensity across a range of m/z values. It looks like a series of small, connected mountain ranges.</p>
<ul>
<li><p><strong>What it is:</strong> Centroiding is the process of converting this “profile” data into a simple list of discrete peaks. The algorithm finds the center of each “mountain” and represents it with a single, precise m/z value and its maximum intensity (the height of the peak).</p></li>
<li><p><strong>Analogy:</strong></p>
<ul>
<li><p><strong>Profile Data:</strong> A detailed topographical map of a mountain range.</p></li>
<li><p><strong>Centroided Data:</strong> A simple list containing only the exact coordinates and altitude of each mountain’s summit.</p></li>
</ul></li>
<li><p><strong>Why it’s done:</strong></p>
<ol type="1">
<li><p><strong>Massive Data Reduction:</strong> It drastically reduces the size of the data file by storing only the most important information.</p></li>
<li><p><strong>Simplification:</strong> It provides a clean “peak list” (m/z and intensity) that is easy for downstream algorithms to work with.</p></li>
</ol></li>
</ul>
</section>
<section id="step-2-de-isotoping去同位素" class="level3">
<h3 class="anchored" data-anchor-id="step-2-de-isotoping去同位素">Step 2: De-isotoping(去同位素)</h3>
<p>This is a clever and crucial step that relies on basic chemistry. Most elements, particularly carbon, have naturally occurring heavy isotopes (e.g., Carbon-13 in addition to the common Carbon-12).</p>
<ul>
<li><p><strong>What it is:</strong> Because of these isotopes, a single peptide does not appear as a single peak in an MS1 scan. Instead, it appears as a characteristic pattern of several peaks, called an “isotope envelope.(同位素包络)” The first peak (M) is the one with all light isotopes (e.g., all Carbon-12). The next peak (M+1) is where one atom is a heavy isotope, and so on.</p></li>
<li><p><strong>The Software’s Job:</strong> The de-isotoping algorithm recognizes these characteristic patterns. It does two things:</p>
<ol type="1">
<li><p>It collapses the entire isotope envelope into a single representative peak, the “monoisotopic peak(单同位素峰)” (the one with the lightest mass). This further simplifies the spectrum.</p></li>
<li><p>It calculates the <strong>charge state (z)</strong> of the peptide ion. The distance between the peaks in the isotope envelope is equal to 1/z. For example, if the peaks are separated by ~0.5 m/z, the charge state is 2+. This is <strong>critically important</strong> information for the next stage.</p></li>
</ol></li>
<li><p><strong>Why it’s done:</strong> To identify a peptide, you need its <strong>neutral mass</strong>. The instrument measures mass-to-charge (m/z). By determining the charge (z) from de-isotoping, the software can now calculate the actual mass: <strong>Mass = (m/z * z) - (mass of protons)</strong>. The search engine needs this neutral mass for its database lookup.</p></li>
</ul>
</section>
<section id="step-3-noise-reduction" class="level3">
<h3 class="anchored" data-anchor-id="step-3-noise-reduction">Step 3: Noise Reduction</h3>
<ul>
<li><p><strong>What it is:</strong> Raw mass spectra contain a lot of low-level background signals. This can be electronic noise from the detector or low-level chemical contaminants. This is often called “grass” because it looks like a lawn of tiny peaks.</p></li>
<li><p><strong>The Software’s Job:</strong> The software applies a simple intensity threshold. Any peaks below this threshold are considered noise and are discarded from the spectrum.</p></li>
<li><p><strong>Why it’s done:</strong> This focuses the analysis on the most significant signals, preventing the database search engine from wasting time trying to match random noise, which can sometimes lead to false positives.</p></li>
</ul>
<hr>
</section>
</section>
<section id="the-search-engine---the-matching-game." class="level2">
<h2 class="anchored" data-anchor-id="the-search-engine---the-matching-game.">The Search Engine - The Matching Game.👻</h2>
<p><em>At this point, we have a clean MS/MS spectrum. It is a list of m/z values for the fragment ions. Think of this spectrum as a&nbsp;<strong>fingerprint left at a crime scene.</strong>&nbsp;We also have the very precise mass of the person who left the fingerprint (the precursor mass from the MS1 scan).</em></p>
<section id="step-1-get-the-list-of-suspects-the-protein-database" class="level3">
<h3 class="anchored" data-anchor-id="step-1-get-the-list-of-suspects-the-protein-database">Step 1: Get the List of Suspects (The Protein Database)</h3>
<p>You cannot identify a peptide unless you know what peptides are possible in your sample. This information comes from a protein sequence database.</p>
<ul>
<li><p><strong>What it is:</strong> A simple text file in <strong>FASTA format</strong>. Each entry contains a header line (starting with &gt;) with the protein’s name and information, followed by the lines of its amino acid sequence (e.g., M G A Q T S…).</p></li>
<li><p><strong>Where you get it:</strong> For most organisms, you download the entire known proteome from a public repository like <strong>UniProt</strong> or <strong>NCBI RefSeq</strong>.</p></li>
<li><p><strong>The Golden Rule:</strong> You <strong>must</strong> use a <em>database that matches your sample’s species</em>. If you are analyzing a mouse sample, you use the mouse proteome. If you search a human sample against a yeast database, you will find nothing.</p></li>
</ul>
</section>
<section id="step-2-create-a-master-list-of-all-possible-peptides-in-silico-digestion" class="level3">
<h3 class="anchored" data-anchor-id="step-2-create-a-master-list-of-all-possible-peptides-in-silico-digestion">Step 2: Create a Master List of All Possible Peptides (In Silico Digestion)</h3>
<p>Proteins are too big; the mass spec analyzes peptides. In the lab, the scientist used an enzyme (almost always <strong>Trypsin(胰蛋白酶)</strong>) to cut the proteins into smaller peptides. Trypsin has a very specific rule: it cuts the protein chain after the amino acids Lysine (K) and Arginine (R).</p>
<p>The search engine simulates this process computationally. This is called in silico digestion (meaning “done in the computer”).</p>
<ul>
<li><p><strong>The Process:</strong> The software reads every single protein sequence in the FASTA file and “cuts” it according to the Trypsin rule.</p></li>
<li><p><strong>The Result:</strong> It generates a massive, theoretical list of every single peptide that could possibly be created from all the proteins in the database. This list can contain millions of peptide sequences. The software also calculates the exact neutral mass of each of these theoretical peptides.</p></li>
</ul>
</section>
<section id="step-3-predict-the-fingerprint-for-each-possible-peptide-theoretical-spectrum-generation" class="level3">
<h3 class="anchored" data-anchor-id="step-3-predict-the-fingerprint-for-each-possible-peptide-theoretical-spectrum-generation">Step 3: Predict the Fingerprint for Each Possible Peptide (Theoretical Spectrum Generation)</h3>
<p>Now for the clever part. For every peptide in that massive list, the software predicts what its MS/MS fingerprint should look like.</p>
<ul>
<li><p><strong>The Process:</strong> When a peptide is fragmented in the mass spectrometer, its backbone breaks in predictable places. This creates two main series of fragment ions, called <strong>b-ions</strong> and <strong>y-ions</strong>. The search engine knows the mass of every amino acid, so for a given peptide sequence (e.g., V G A Q T S K), it can calculate the precise m/z of all the possible b-ions and y-ions that would be formed if that peptide were fragmented.</p></li>
<li><p><strong>The Result:</strong> A “theoretical spectrum” for every single peptide in the master list.</p></li>
</ul>
</section>
<section id="step-4-the-match-and-score" class="level3">
<h3 class="anchored" data-anchor-id="step-4-the-match-and-score">Step 4: The Match and Score</h3>
<p>Now the detective work begins. The search engine takes one of your <strong>experimental MS/MS spectra</strong> and tries to find the best match in the theoretical library it just created.</p>
<ol type="1">
<li><p><strong>Filtering by Mass:</strong> First, it uses the precursor mass to filter the list. If your experimental precursor ion had a mass of 874.48 Daltons, the software will immediately discard all theoretical peptides that don’t have a mass extremely close to that (e.g., within a 10 ppm window). This is a huge and essential filtering step.</p></li>
<li><p><strong>Comparing Spectra:</strong> For the handful of candidate peptides that passed the mass filter, the software compares their theoretical spectra (the predicted b- and y-ions) against the peaks in your experimental spectrum.</p></li>
<li><p><strong>Scoring:</strong> An algorithm calculates a score based on how good the match is. A higher score is given if:</p>
<ul>
<li><p>More of the experimental peaks are explained by the theoretical fragments.</p></li>
<li><p>The matched peaks are the most intense peaks in the experimental spectrum.</p></li>
<li><p>Long, consecutive series of b- or y-ions are found.</p></li>
</ul></li>
</ol>
<p>Different search engines (<strong>Mascot, Sequest, Andromeda</strong>) use different scoring algorithms, but the principle is the same.</p>
</section>
<section id="the-final-output-the-peptide-spectrum-match-psm" class="level3">
<h3 class="anchored" data-anchor-id="the-final-output-the-peptide-spectrum-match-psm">The Final Output: The Peptide-Spectrum Match (PSM)</h3>
<p>After comparing all candidate peptides, the search engine makes a final decision. It declares the theoretical peptide that received the highest score as the best match. This single assignment is called a <strong>Peptide-Spectrum Match (PSM)</strong>.</p>
<p>The final output of this stage is a huge table that looks something like this:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 26%">
<col style="width: 33%">
<col style="width: 15%">
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Scan Number</strong></td>
<td><strong>Precursor m/z</strong></td>
<td><strong>Best Peptide Match</strong></td>
<td><strong>Score</strong></td>
</tr>
<tr class="even">
<td>5024</td>
<td>643.32</td>
<td>VAGISSAK</td>
<td>123.4</td>
</tr>
<tr class="odd">
<td>5028</td>
<td>712.38</td>
<td>LKECCDKPLLEK</td>
<td>98.7</td>
</tr>
<tr class="even">
<td>5031</td>
<td>598.29</td>
<td>FSTVAGESGSADIK</td>
<td>154.2</td>
</tr>
</tbody>
</table>
<p>We now have a preliminary identity for thousands of our spectra. But a crucial question remains: just because a peptide got the “best score,” does that mean it’s the correct identification? This is a huge problem of false positives, which brings us to our next, critically important lesson on statistical validation.</p>
<p><em>In the last lesson, the search engine gave us a huge list of Peptide-Spectrum Matches (PSMs), each with a score. It is incredibly tempting to just take the highest-scoring match for each spectrum and declare victory.</em></p>
<p><strong>This would be a catastrophic mistake.</strong></p>
</section>
<section id="the-problem-the-inevitability-of-false-positives" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-the-inevitability-of-false-positives">The Problem: The Inevitability of False Positives</h3>
<ul>
<li><p><strong>The Search Engine is Naive:</strong> A search engine will always return the best-scoring match it can find, even if that “best” match is complete nonsense. <strong><em>It has no concept of “I don’t know.”</em></strong></p></li>
<li><p><strong>Random Chance is Powerful:</strong> Your experimental spectra are imperfect, and your database of theoretical peptides is enormous (millions of entries). By sheer random chance, a noisy, unidentifiable spectrum will look somewhat similar to some random peptide in the database.</p></li>
<li><p><strong>The Result:</strong> A significant portion of your high-scoring PSMs are likely to be <strong>false positives</strong>—incorrect matches that are purely the result of random chance.</p></li>
</ul>
<p>Without a statistical method to control for this, your data is unusable. You have a list of “identifications” but no idea which ones are real and which are garbage.</p>
</section>
<section id="the-solution-the-target-decoy-strategy" class="level3">
<h3 class="anchored" data-anchor-id="the-solution-the-target-decoy-strategy">The Solution: The Target-Decoy Strategy</h3>
<p>To solve this, the entire field of proteomics uses an elegant and powerful statistical method called the <strong>Target-Decoy Strategy.</strong> Understanding this is non-negotiable for a bioinformatician.</p>
<p>Let’s use an analogy. Imagine you are searching a huge English dictionary (<strong>the Target</strong>) for a list of random letter scrambles (your spectra). You will definitely find some real words just by chance. How do you know how many of your “hits” are just random luck?</p>
<p>The answer: you create a <strong>Decoy</strong> dictionary where every single word is spelled backward (“proteomics” becomes “scimoetorp”). This decoy dictionary is full of nonsense. You then search for your letter scrambles in a combined dictionary of real + nonsense words.</p>
<p>The number of “matches” you find in the nonsense, backward dictionary is a direct and excellent estimate of how many random-chance matches you are getting in the real dictionary.</p>
<p><strong>This is exactly how it works in proteomics:</strong></p>
<p><strong>Step 1: Create the Decoy Database</strong></p>
<ul>
<li><p>Before the search, the software takes your protein FASTA file (<strong>Target database</strong>) and creates a second, decoy database.</p></li>
<li><p>The most common method is to simply <strong>reverse</strong> the sequence of every protein. For example:</p>
<ul>
<li><p><strong>Target:</strong> MGTSAQ…</p></li>
<li><p><strong>Decoy:</strong> …QASTGM</p></li>
</ul></li>
<li><p>This creates a database of nonsense proteins that still have the exact same amino acid composition and mass distribution as the real ones, making it a perfect statistical model for random matches.</p></li>
</ul>
<p><strong>Step 2: A Competitive Search</strong></p>
<ul>
<li><p>The software combines the Target and Decoy databases into one large file.</p></li>
<li><p>It then runs the database search against this combined database.</p></li>
<li><p>Crucially, the search engine does not know which is which. It treats them all as valid possibilities.</p></li>
<li><p>Each of your experimental spectra will now be matched to either a Target peptide or a Decoy peptide, whichever gives the better score.</p></li>
</ul>
<p><strong>Step 3: Calculate the False Discovery Rate (FDR)</strong></p>
<ul>
<li><p>After the search, you have your list of all PSMs, each labeled as a Target or a Decoy.</p></li>
<li><p>You rank all of them, from highest score to lowest score.</p></li>
<li><p>Now, you go down the list and ask a simple question: “At this score cutoff, how many of my hits are nonsense (Decoys)?”</p></li>
<li><p>The logic is simple but powerful: <strong>The number of decoy matches at a given score threshold is a great estimate of the number of false-positive target matches at that same threshold.</strong></p></li>
<li><p>The <strong>False Discovery Rate (FDR)</strong> is calculated as:<br>
FDR = (Number of Decoy Matches) / (Number of Target Matches)</p></li>
</ul>
<p><strong>Step 4: Apply the Cutoff</strong></p>
<ul>
<li><p>The universal standard in proteomics is to accept a <strong>1% FDR</strong>.</p></li>
<li><p>The software goes down the ranked list of PSMs until it finds the score cutoff where the FDR is exactly 1%. For example, it might find that at a score of 135.2, there are 10,000 Target hits and 100 Decoy hits.</p>
<ul>
<li>FDR = 100 / 10,000 = 0.01 = 1%</li>
</ul></li>
<li><p>This score (135.2) now becomes your quality threshold. <strong>You discard all PSMs with a score below this value.</strong></p></li>
</ul>
</section>
<section id="summary-for-this-lesson-1" class="level3">
<h3 class="anchored" data-anchor-id="summary-for-this-lesson-1">Summary for this Lesson</h3>
<p>By using the Target-Decoy strategy, you have transformed your dataset from a raw list of “best guesses” into a statistically validated set of identifications.</p>
<p>When you say, “I have 10,000 peptide identifications at a 1% FDR,” you are making a clear, scientifically defensible statement: <strong>“Here is my list of identified peptides. I am confident that, at most, 1% of these are incorrect.”</strong></p>
<p>This step is the bedrock of reliable proteomics. It is not optional. It is the bioinformatician’s primary responsibility to ensure it is performed correctly.</p>
<hr>
</section>
</section>
<section id="the-protein-inference-problem---from-peptides-to-proteins" class="level2">
<h2 class="anchored" data-anchor-id="the-protein-inference-problem---from-peptides-to-proteins">The Protein inference Problem - from peptides to proteins 👹</h2>
<p><strong>Part 2, Lesson 6: The Protein Inference Problem - From Peptides to Proteins.</strong></p>
<section id="goal-understand-the-challenge-of-accurately-identifying-proteins-from-a-list-of-peptides-and-the-logic-used-to-solve-it." class="level3">
<h3 class="anchored" data-anchor-id="goal-understand-the-challenge-of-accurately-identifying-proteins-from-a-list-of-peptides-and-the-logic-used-to-solve-it.">Goal: Understand the challenge of accurately identifying proteins from a list of peptides and the logic used to solve it.</h3>
<p>This seems like it should be an easy step. If you found a peptide from Protein A, then Protein A must be in your sample, right?</p>
<p>The answer is: <strong>maybe.</strong> The problem is that your list of identified peptides is often ambiguous.</p>
</section>
<section id="the-core-problem-ambiguity歧义-from-shared-peptides" class="level3">
<h3 class="anchored" data-anchor-id="the-core-problem-ambiguity歧义-from-shared-peptides">The Core Problem: Ambiguity(歧义) from Shared Peptides</h3>
<p>A single peptide sequence can, and often does, belong to more than one protein in the database. This is the central challenge of protein inference.</p>
<p>Why does this happen?</p>
<ol type="1">
<li><p><strong>Protein Isoforms(异构体):</strong> A single gene can produce multiple, slightly different versions of a protein through alternative splicing(剪切). These isoforms might be 95% identical. Any peptide identified from the shared region will map to all of those isoforms.</p></li>
<li><p><strong>Homologous Proteins(同源蛋白0:</strong> Organisms have protein families with very similar sequences (e.g., Actin(肌动蛋白)-Beta and Actin-Gamma). A peptide from a conserved region of the protein could match both.</p></li>
<li><p><strong>Database Redundancy:</strong> The FASTA database might contain slightly different entries for the same protein.</p></li>
</ol>
<p>This ambiguity means you cannot simply make a list of every protein that one of your peptides hits. You would massively over-report the number of proteins and include many false positives.</p>
</section>
<section id="the-solution-the-principle-of-parsimony简约-occams-razor奥卡姆剃刀" class="level3">
<h3 class="anchored" data-anchor-id="the-solution-the-principle-of-parsimony简约-occams-razor奥卡姆剃刀">The Solution: The Principle of Parsimony(简约) (Occam’s Razor(奥卡姆剃刀))</h3>
<p>To solve this, proteomics software applies a logical principle called <strong>parsimony</strong>, also known as Occam’s Razor: <strong>“The simplest explanation is the best.”</strong></p>
<p>In proteomics, this translates to: <strong>“Explain all of your identified peptide evidence with the minimum possible number of proteins.”(用尽可能少的蛋白质解释所有已鉴定的肽证据)</strong></p>
<p>Let’s walk through this with a simple analogy.</p>
<ul>
<li><p><strong>Peptides</strong> are like unique words.</p></li>
<li><p><strong>Proteins</strong> are like books.</p></li>
<li><p>Your experiment gives you a list of words, and you have to figure out which books were on the shelf.</p></li>
</ul>
<p><strong>Scenario:</strong></p>
<ul>
<li><p>You find the word <strong>“photosynthesis(光合作用)”</strong>. This word only appears in the “Biology Textbook”. Conclusion: You definitely have the Biology Textbook. (“Photosynthesis” is a <strong>unique</strong> or <strong>proteotypic peptide</strong>).</p></li>
<li><p>You find the word <strong>“the”</strong>. This word appears in the “Biology Textbook,” the “History Book,” and the “Chemistry Book”. Conclusion: You can’t be sure which book it came from. (“The” is a <strong>shared</strong> or <strong>degenerate peptide</strong>).</p></li>
</ul>
<p><strong>How Parsimony Works in Practice:</strong></p>
<p>Imagine we identify three peptides with the following evidence:</p>
<ul>
<li><p><strong>Peptide 1:</strong> Maps only to Protein A. (Unique)</p></li>
<li><p><strong>Peptide 2:</strong> Maps only to Protein B. (Unique)</p></li>
<li><p><strong>Peptide 3:</strong> Maps to both Protein A and Protein B. (Shared)</p></li>
</ul>
<p>The software reasons as follows:</p>
<ol type="1">
<li><p>“I have definitive evidence for Protein A because I found Peptide 1.”</p></li>
<li><p>“I have definitive evidence for Protein B because I found Peptide 2.”</p></li>
<li><p>“Now I need to explain Peptide 3. I already know that Protein A and Protein B are both present. Since Peptide 3 belongs to both, its presence is already explained. I do not need to invoke any new proteins to explain this evidence.”</p></li>
</ol>
<p>The final conclusion is that both Protein A and Protein B are confidently identified.</p>
<p><strong>Now, a trickier case:</strong> What if we only found Peptide 1 and Peptide 3?</p>
<ol type="1">
<li><p>“I have definitive evidence for Protein A because I found Peptide 1.”</p></li>
<li><p>“Now I need to explain Peptide 3. It could come from Protein A or Protein B. However, the simplest explanation (parsimony) is to say it also came from Protein A, which I already know is present. I don’t have enough evidence to justify also claiming that Protein B is present.”</p></li>
</ol>
<p>In this second case, the software would <strong><em>only report Protein A as a high-confidence identification</em></strong>. Protein B would not be reported because its presence is not required to explain the observed peptides.</p>
</section>
<section id="the-output-protein-groups" class="level3">
<h3 class="anchored" data-anchor-id="the-output-protein-groups">The Output: Protein Groups</h3>
<p>Because of this ambiguity, the final output of the inference step is often not a simple list of proteins, but a list of <strong>Protein Groups</strong>.</p>
<p>A protein group is a list of one or more proteins that cannot be distinguished based on the set of identified peptides.</p>
<ul>
<li><p>If you only identify shared peptides that map to Proteins A, B, and C, your software will report one line for the protein group “(A, B, C)”. This tells you that at least one of these proteins is present, but you can’t be sure which.</p></li>
<li><p>The first protein listed in a group is often the “master protein” – the one with the most peptide evidence or the best-known entry in the database.</p></li>
</ul>
<p>As a bioinformatician, you must understand that when you see a “protein identification,” you are often looking at a “protein group,” which represents the software’s best, most parsimonious(简约) conclusion about the protein-level evidence.</p>
</section>
<section id="summary-for-this-lesson-2" class="level3">
<h3 class="anchored" data-anchor-id="summary-for-this-lesson-2">Summary for this Lesson</h3>
<ul>
<li><p>Protein inference is necessary because peptides can be shared between multiple proteins.</p></li>
<li><p>The guiding logic is the <strong>Principle of Parsimony</strong>: explain the data with the fewest proteins possible.</p></li>
<li><p>The process relies on first identifying proteins that have <strong>unique peptide</strong> evidence.</p></li>
<li><p>The final output is a list of <strong>protein groups</strong> that represents the resolved (and sometimes unresolved) ambiguity.</p></li>
</ul>
<hr>
</section>
</section>
<section id="quantification" class="level2">
<h2 class="anchored" data-anchor-id="quantification">Quantification😋</h2>
<p><strong>Part 2, Lesson 7: Quantification - How Much Is There?</strong></p>
<section id="goal-learn-the-computational-methods-for-determining-the-abundance-of-proteins." class="level3">
<h3 class="anchored" data-anchor-id="goal-learn-the-computational-methods-for-determining-the-abundance-of-proteins.">Goal: Learn the computational methods for determining the abundance of proteins.</h3>
<p>Identifying proteins is only half the story. In most experiments, the real goal is to find out which proteins have changed in abundance between different conditions (e.g., healthy vs.&nbsp;disease). This requires <strong>quantification</strong>.</p>
<p>The fundamental principle of quantification in mass spectrometry is: <strong>“<em>The more of a peptide you have, the stronger its signal will be.</em>”</strong></p>
<p>Our job as bioinformaticians is to use software to extract this signal for every peptide and then roll it up to the protein level. There are two main families of techniques for this.</p>
</section>
<section id="method-1-label-free-quantification-lfq无定量标记" class="level3">
<h3 class="anchored" data-anchor-id="method-1-label-free-quantification-lfq无定量标记">Method 1: Label-Free Quantification (LFQ)(无定量标记)</h3>
<p>This is the most conceptually straightforward method. As the name implies, you do not add any special chemical tags or labels to your samples. You just run them one by one and compare the signals.</p>
<ul>
<li><p><strong>The Data Source:</strong> LFQ relies entirely on the <strong>MS1 scans</strong> – the high-resolution survey scans we discussed in Lesson 1.</p></li>
<li><p><strong>The Computational Workflow (MaxLFQ is the most famous algorithm):</strong></p>
<ol type="1">
<li><p><strong>Feature Detection:</strong> The software goes through all the MS1 scans and creates a 3D map for every peptide ion it finds: its <strong>m/z</strong>, its <strong>retention time</strong>, and its <strong>intensity</strong>. This 3D peak is called a “feature.”</p></li>
<li><p><strong>Alignment:</strong> The retention time for a peptide is never perfectly stable; it can drift slightly from run to run. The software must perform <strong>retention time alignment</strong> to <strong>warp the time axis of each run so that the same peptide feature appears at the same time in all samples</strong>. This is a critical step.</p></li>
<li><p><strong>“Match Between Runs”:</strong> This is a key feature to reduce missing values. <em>Imagine Peptide X was identified by MS/MS in Sample 1 but not in Sample 2 (perhaps because it wasn’t intense enough to be selected for fragmentation in that run).</em> The software knows the m/z and aligned retention time of Peptide X. It can go back to the MS1 map of Sample 2 and say, “Is there a feature at this exact coordinate?” If it finds one, it can “<strong><em>transfer</em></strong>” the identification and quantify the feature, even without an MS/MS scan in that specific run.</p></li>
<li><p><strong>Intensity Extraction:</strong> The software calculates the <strong>Area Under the Curve (AUC)(曲线下面积)</strong> for each feature. T<strong><em>his total ion current is the peptide’s raw abundance measurement.</em></strong></p></li>
<li><p><strong>Protein Quantification:</strong> To get the protein’s abundance, the software takes the intensities of all the unique peptides belonging to that protein and uses a robust algorithm (like summing the top 3 most intense peptides or using a median) to calculate a single, representative value for the protein’s abundance.</p></li>
</ol></li>
<li><p><strong>Pros:</strong> Simple lab workflow, inexpensive.</p></li>
<li><p><strong>Cons:</strong> Highly dependent on instrument stability; more prone to missing values than labeling methods.</p></li>
</ul>
</section>
<section id="method-2-isobaric-labeling-tmt-and-itraq同量异位素标记" class="level3">
<h3 class="anchored" data-anchor-id="method-2-isobaric-labeling-tmt-and-itraq同量异位素标记">Method 2: Isobaric Labeling (TMT and iTRAQ)(同量异位素标记)</h3>
<p>This is a more complex but very powerful chemical method that allows you to run multiple samples at the same time.This is more needing the help of wetlab.</p>
<ul>
<li><p><strong>The Lab Workflow:</strong> In the lab, the scientist takes up to 16 different samples (e.g., 8 healthy, 8 disease). Each sample is labeled with a different chemical tag (e.g., TMT-126, TMT-127N, etc.). These tags are <strong>isobaric</strong>, <strong><em>meaning they have the exact same total mass.</em></strong> After labeling, all 16 samples are mixed together into a single tube and run as one mass spec experiment.</p></li>
<li><p><strong>The Data Source:</strong></p>
<ul>
<li><p><strong>Identification:</strong> Comes from the MS/MS spectrum, just like before.</p></li>
<li><p><strong>Quantification:</strong> This is the clever part. <strong><em>When a tagged peptide is fragmented in the mass spectrometer (during an MS2 or a special MS3 scan), the tags break off. This produces small, low-mass</em></strong> <strong>“reporter ions.”</strong> <strong><em>Each tag produces a reporter ion with a unique mass (e.g., 126, 127, etc.).</em></strong></p></li>
</ul></li>
<li><p><strong>The Computational Workflow:</strong></p>
<ol type="1">
<li><p><strong>Identification:</strong> First, the software identifies the peptide sequence from the higher-mass fragment ions in the MS/MS spectrum (the b- and y-ions).</p></li>
<li><p><strong>Reporter Ion Extraction:</strong> Once a peptide is identified, the software looks in the <strong><em>low-mass region of that same MS/MS spectrum</em></strong>. It finds the special reporter ion peaks.</p></li>
<li><p><strong>Relative Quantification(相对定量):</strong> The <strong>intensity of each reporter ion peak</strong> is <strong>directly proportional to the amount of that peptide that came from the original sample(每个报告离子峰的强度与原始样品中该肽的含量成正比)</strong>. If the 126 reporter is twice as intense as the 127N reporter, it means there was twice as much of that peptide in Sample 1 than in Sample 2.</p></li>
<li><p><strong>Protein Quantification:</strong> Just like with LFQ, the relative ratios from all the peptides belonging to a protein are combined to get a final set of ratios for the protein.</p></li>
</ol></li>
<li><p><strong>Pros:</strong> Very <strong><em>precise relative quantification</em></strong>; fewer missing values because all samples are mixed.</p></li>
<li><p><strong>Cons:</strong> More expensive and complex lab work; can suffer from “ratio compression” issues.</p></li>
</ul>
</section>
<section id="final-critical-step-for-both-methods-normalization" class="level3">
<h3 class="anchored" data-anchor-id="final-critical-step-for-both-methods-normalization">Final Critical Step for Both Methods: Normalization</h3>
<p>You can <strong>never</strong> directly compare the raw intensity values between different runs or different TMT channels. There will always be small variations (e.g., one sample was slightly more concentrated, the instrument spray was slightly stronger for one run).</p>
<ul>
<li><p><strong>What it is:</strong> Normalization is a mathematical adjustment that corrects for these systematic, non-biological variations.</p></li>
<li><p><strong>How it works:</strong> Many methods exist, but a common one is <strong>median normalization</strong>. The software assumes that for most proteins, their abundance does not change. It calculates the median of all protein intensities (or ratios) in each sample/channel and adjusts the values so that the medians are all equal.</p></li>
<li><p><strong>Why it matters:</strong> Without normalization, you might think a protein is upregulated in one sample when in reality, you just loaded more of that entire sample onto the instrument. <strong>Normalization is absolutely essential before any downstream statistical analysis.</strong></p></li>
</ul>
</section>
<section id="summary-for-this-lesson-3" class="level3">
<h3 class="anchored" data-anchor-id="summary-for-this-lesson-3">Summary for this Lesson</h3>
<ul>
<li><p>Quantification connects a signal intensity to a peptide/protein.</p></li>
<li><p><strong>LFQ</strong> uses the area of the peptide’s peak in the <strong>MS1 scan</strong>. It requires careful <strong>retention time alignment</strong>.</p></li>
<li><p><strong>Isobaric Labeling (TMT)</strong> uses the intensity of special <strong>reporter ions</strong> in the <strong>MS/MS (or MS3) scan</strong>.</p></li>
<li><p>The final step before any biological analysis is <strong>normalization</strong>, which corrects for technical variability between samples.</p></li>
</ul>
<hr>
</section>
</section>
<section id="downstream-analysis" class="level2">
<h2 class="anchored" data-anchor-id="downstream-analysis">🐗DownStream analysis🐻</h2>
<hr>
</section>
<section id="downstream-statistics" class="level2">
<h2 class="anchored" data-anchor-id="downstream-statistics">Downstream Statistics🙂‍↔︎️</h2>
<p><strong>Part 3, Lesson 8: Downstream Statistics - Finding What’s Interesting.</strong></p>
<section id="goal-learn-how-to-find-statistically-significant-changes-between-your-sample-groups-from-your-quantified-protein-list." class="level3">
<h3 class="anchored" data-anchor-id="goal-learn-how-to-find-statistically-significant-changes-between-your-sample-groups-from-your-quantified-protein-list.">Goal: Learn how to find statistically significant changes between your sample groups from your quantified protein list.</h3>
<p>At this point, you have a giant table. The rows are your proteins, and the columns are your samples. The cells contain the normalized abundance (e.g., LFQ intensity or TMT ratio) for each protein in each sample.</p>
<p>Our goal is to compare the abundance values between our experimental groups (e.g., “Control” vs.&nbsp;“Treatment”) and find the proteins that show a real, significant difference.</p>
</section>
<section id="step-1-handling-missing-values-data-imputation" class="level3">
<h3 class="anchored" data-anchor-id="step-1-handling-missing-values-data-imputation">Step 1: Handling Missing Values (Data Imputation)</h3>
<p>Especially in Label-Free Quantification (LFQ), your data table will have holes in it. A protein might be quantified in all three “Control” replicates but only one of the “Treatment” replicates. This is a huge problem for most statistical tests.</p>
<ul>
<li><p><strong>Why do values go missing?</strong> Often, it’s because the peptide’s signal was too low to be reliably detected in that run. This means the missing values are not random; they are often linked to low abundance.</p></li>
<li><p><strong>The Solution: Imputation(插补).</strong> Imputation is the process of filling in the missing values with a reasonable estimate. You can’t just ignore them.</p>
<ul>
<li><strong>A Common Method:</strong> A popular approach is to impute the missing values <strong><em>with a small number</em></strong> drawn from the very low end of the overall intensity distribution. The logic is that the value is missing because it was below the detection limit, so we should replace it with a value that reflects that.</li>
</ul></li>
<li><p><strong>The Bioinformatician’s Role:</strong> You need to be aware that imputation is happening, understand the method your software is using, and recognize how it might affect your results.</p></li>
</ul>
</section>
<section id="step-2-log-transformation" class="level3">
<h3 class="anchored" data-anchor-id="step-2-log-transformation">Step 2: Log Transformation</h3>
<p>Protein abundance data often spans a huge dynamic range (from very low to very high intensity). This kind of data is usually <strong><em>not normally distributed</em></strong> (it doesn’t form a nice bell curve), <strong><em>which violates the assumptions of many statistical tests.</em></strong></p>
<ul>
<li><p><strong>The Solution:</strong> You perform a <strong>log transformation</strong> on your data (usually log base 2, or log2).</p></li>
<li><p><strong>Why it’s done:</strong></p>
<ol type="1">
<li><p><strong>Normalization:</strong> It makes the data more symmetric and closer to a normal distribution, which is better for statistics.</p></li>
<li><p><strong>Meaningful Fold Changes:</strong> A log2 transformation makes fold changes intuitive(直观).</p>
<ul>
<li><p>An increase of 2-fold becomes log2(2) = 1.</p></li>
<li><p>An increase of 4-fold becomes log2(4) = 2.</p></li>
<li><p>A decrease of 2-fold (i.e., half) becomes log2(0.5) = -1.</p></li>
<li><p>No change (1-fold) is log2(1) = 0.<br>
This symmetry makes plotting and interpretation much easier.</p></li>
</ul></li>
</ol></li>
</ul>
</section>
<section id="step-3-performing-the-statistical-test" class="level3">
<h3 class="anchored" data-anchor-id="step-3-performing-the-statistical-test">Step 3: Performing the Statistical Test</h3>
<p>Now you can finally compare your groups.</p>
<ul>
<li><p><strong>The Tools:</strong> You’ll use standard statistical tests to see if the average abundance in one group is significantly different from the average in the other.</p>
<ul>
<li><p>If you have two groups (e.g., Control vs.&nbsp;Treatment): Use a <strong>Student’s t-test</strong>.</p></li>
<li><p>If you have more than two groups (e.g., Control vs.&nbsp;Treatment A vs.&nbsp;Treatment B): Use <strong>ANOVA</strong> (Analysis of Variance).</p></li>
</ul></li>
<li><p><strong>The Output:</strong> For every single protein, the test will give you two important numbers:</p>
<ol type="1">
<li><p><strong>p-value:</strong> The probability that the difference you observed between the groups is just due to random chance. A small p-value (e.g., &lt; 0.05) suggests the difference is real.</p></li>
<li><p><strong>Fold Change (or log2 Fold Change):</strong> How big the difference is. A fold change of 2 means the protein is twice as abundant in one group.</p></li>
</ol></li>
</ul>
</section>
<section id="step-4-the-multiple-hypothesis-testing-problem-and-correction" class="level3">
<h3 class="anchored" data-anchor-id="step-4-the-multiple-hypothesis-testing-problem-and-correction">Step 4: The Multiple Hypothesis Testing Problem and Correction</h3>
<p>You just performed thousands of t-tests simultaneously (one for each protein). This leads to a major statistical trap.</p>
<ul>
<li><p><strong>The Problem:</strong> If you use a p-value cutoff of 0.05, you are accepting a 5% chance of a false positive for each test. <strong><em>If you do 10,000 tests</em></strong>, you would expect 10,000 * 0.05 = <strong><em>500 proteins to look significant just by pure random luck!</em></strong></p></li>
<li><p><strong>The Solution: p-value Adjustment.</strong> You must correct for this multiple testing. The most common method in bioinformatics is the <strong>Benjamini-Hochberg procedure</strong>.</p></li>
<li><p><strong>The Output:</strong> This method takes your list of raw p-values and calculates an <strong>“adjusted p-value”</strong> or a <strong>“q-value.”</strong> This q-value is essentially the False Discovery Rate (FDR) for your list of significant proteins.</p></li>
<li><p><strong>The New Rule:</strong> Instead of using a p-value &lt; 0.05, you will now use a <strong>q-value &lt; 0.05</strong> (or 0.01) to decide which proteins are truly significant.</p></li>
</ul>
</section>
<section id="step-5-visualizing-the-results-the-volcano-plot" class="level3">
<h3 class="anchored" data-anchor-id="step-5-visualizing-the-results-the-volcano-plot">Step 5: Visualizing the Results: The Volcano Plot</h3>
<p>You now have a list of thousands of proteins, each with a log2 fold change and an adjusted p-value (q-value). The best way to visualize this is with a <strong>Volcano Plot</strong>.</p>
<ul>
<li><p><strong>X-axis:</strong> The log2(Fold Change). Proteins to the right are upregulated; proteins to the left are downregulated.</p></li>
<li><p><strong>Y-axis:</strong> The -log10(adjusted p-value). The minus sign is just to flip the plot. A smaller p-value (more significant) becomes a larger -log10 value, so the most significant proteins are at the top.</p></li>
<li><p><strong>Interpretation:</strong></p>
<ul>
<li><p>You draw two vertical lines for a fold-change cutoff (e.g., at -1 and +1, for a 2-fold change).</p></li>
<li><p>You draw one horizontal line for your significance cutoff (e.g., at -log10(0.05)).</p></li>
<li><p><strong>The “hits” are the points in the top-left and top-right corners.</strong> These are the proteins that are both statistically significant AND have a large magnitude of change. The points in the middle are either not significant or did not change much.</p></li>
</ul></li>
</ul>
</section>
<section id="summary-for-this-lesson-4" class="level3">
<h3 class="anchored" data-anchor-id="summary-for-this-lesson-4">Summary for this Lesson</h3>
<p>This is your final data filtering step. You start with a quantified list of all proteins and, through a rigorous statistical workflow, you produce a final, high-confidence list of <strong>differentially abundant proteins</strong>.</p>
<hr>
</section>
</section>
<section id="go-and-kegg" class="level2">
<h2 class="anchored" data-anchor-id="go-and-kegg">Go and KEGG 😏</h2>
<p><strong>Part 3, Lesson 9: Functional Enrichment Analysis - What Does the Protein List Mean?</strong></p>
<section id="goal-go-from-a-list-of-significant-proteins-to-a-coherent-biological-story." class="level3">
<h3 class="anchored" data-anchor-id="goal-go-from-a-list-of-significant-proteins-to-a-coherent-biological-story.">Goal: Go from a list of significant proteins to a coherent biological story.</h3>
<p>Imagine your list of differentially abundant proteins is a random bag of Scrabble tiles. You might have the letters: E, N, Y, M, Z, E. Just looking at the individual letters doesn’t tell you much. But if you rearrange them to spell <strong>“ENZYME,”</strong> you suddenly have a meaningful concept.</p>
<p>Functional enrichment analysis is the computational tool that does this “rearranging” for you. It takes your list of proteins and asks: <strong>“Are there any biological themes, functions, or pathways that are surprisingly common (over-represented) in this list compared to the whole proteome?”</strong></p>
</section>
<section id="the-tools-proteomics-knowledge-bases" class="level3">
<h3 class="anchored" data-anchor-id="the-tools-proteomics-knowledge-bases">The Tools: Proteomics Knowledge Bases</h3>
<p>To do this, we need structured, computer-readable knowledge about what each protein does. This information is stored in large, curated public databases called “knowledge bases.” There are three main types we use.</p>
<p><strong>1. Gene Ontology (GO)</strong></p>
<ul>
<li><p><strong>What it is:</strong> The GO project is a massive effort to describe the functions of genes and proteins in a standardized way. It’s the most widely used resource for this kind of analysis.</p></li>
<li><p><strong>The Three Domains of GO:</strong> It describes each protein in three ways:</p>
<ul>
<li><p><strong>Biological Process (BP):</strong> The larger biological program the protein participates in (e.g., “cell division,” “glycolysis,” “DNA repair”). This is often the most useful category for interpretation.</p></li>
<li><p><strong>Molecular Function (MF):</strong> The specific biochemical activity of the protein (e.g., “protein kinase activity,” “DNA binding,” “transporter activity”).</p></li>
<li><p><strong>Cellular Component (CC):</strong> Where the protein is located in the cell (e.g., “nucleus,” “mitochondrion,” “plasma membrane”).</p></li>
</ul></li>
<li><p><strong>Example:</strong> For the protein Hexokinase, a GO annotation might be: BP: “glycolysis”, MF: “ATP binding”, CC: “cytosol”.</p></li>
</ul>
<p><strong>2. Pathway Databases (KEGG and Reactome)</strong></p>
<ul>
<li><p><strong>What they are:</strong> These databases go a step beyond simple functional labels. They are detailed, hand-drawn maps of known biological pathways.</p>
<ul>
<li><p><strong>KEGG (Kyoto Encyclopedia of Genes and Genomes):</strong> Famous for its iconic, color-coded pathway diagrams showing how metabolites are converted by enzymes.</p></li>
<li><p><strong>Reactome:</strong> Focuses on human biological reactions and pathways in extreme detail, showing all the molecular players and their interactions.</p></li>
</ul></li>
<li><p><strong>Why they are useful:</strong> If you find that many of your upregulated proteins all fall into the “Citric Acid Cycle” pathway in KEGG, you have a very strong and clear biological story.</p></li>
</ul>
</section>
<section id="the-method-the-statistics-of-over-representation" class="level3">
<h3 class="anchored" data-anchor-id="the-method-the-statistics-of-over-representation">The Method: The Statistics of “Over-representation”</h3>
<p>How does the software decide if a theme is “surprisingly common”? It uses a statistical test, most commonly the <strong>Hypergeometric Test</strong> or its close cousin, <strong>Fisher’s Exact Test</strong>.</p>
<p>Let’s use an analogy.</p>
<ul>
<li><p>Imagine a large urn (<strong>the entire proteome of your organism</strong>) containing 10,000 marbles.</p></li>
<li><p>Of these, 100 are red (<strong>all the proteins known to be involved in “DNA repair”</strong>).</p></li>
<li><p>Now, you blindly draw 50 marbles from the urn (<strong>your list of 50 significantly upregulated proteins</strong>).</p></li>
<li><p>In your hand, you find that 10 of your 50 marbles are red.</p></li>
</ul>
<p>The hypergeometric test answers the question: <strong>“If I were to randomly draw 50 marbles from this urn, what is the probability that I would get 10 or more red ones just by pure chance?”</strong></p>
<p>If this probability (the <strong>p-value</strong>) is very, very small (e.g., 1e-10), you can confidently conclude that your selection was not random. You have “enriched” for red marbles. In exactly the same way, the software calculates whether your list of significant proteins is <strong>enriched</strong> for proteins belonging to the “DNA repair” GO term.</p>
</section>
<section id="the-workflow-and-output" class="level3">
<h3 class="anchored" data-anchor-id="the-workflow-and-output">The Workflow and Output</h3>
<ol type="1">
<li><p><strong>Input:</strong> You will use a web-based tool (like <strong>DAVID</strong> or <strong>Metascape</strong>) or a programming package in R/Python. You provide two things:</p>
<ul>
<li><p>Your list of significant protein IDs.</p></li>
<li><p>A “background” or “universe” list, which is all the proteins you identified in your experiment.</p></li>
</ul></li>
<li><p><strong>Analysis:</strong> The tool runs the hypergeometric test for every single GO term and pathway in its database.</p></li>
<li><p><strong>Output:</strong> You get a ranked table of the most significantly enriched terms. The columns will typically be:</p>
<ul>
<li><p><strong>Term Name:</strong> e.g., “GO:0006281 DNA repair”</p></li>
<li><p><strong>p-value:</strong> The result of the hypergeometric test.</p></li>
<li><p><strong>Adjusted p-value (FDR):</strong> Just like in our differential expression analysis, this is essential because you are performing thousands of tests.</p></li>
<li><p><strong>Fold Enrichment:</strong> How much more common the term is in your list compared to the background.</p></li>
<li><p><strong>Gene/Protein IDs:</strong> The specific proteins from your list that belong to that term.</p></li>
</ul></li>
</ol>
</section>
<section id="summary-for-this-lesson-5" class="level3">
<h3 class="anchored" data-anchor-id="summary-for-this-lesson-5">Summary for this Lesson</h3>
<p>Functional enrichment analysis is the bridge from a sterile list of protein names to a vibrant biological narrative. It allows you to move from saying:</p>
<p>“We identified that proteins A, B, X, and Y were upregulated.”</p>
<hr>
</section>
</section>
<section id="network-analysis" class="level2">
<h2 class="anchored" data-anchor-id="network-analysis">Network Analysis🥶</h2>
<p><strong>Part 3, Lesson 10: Network Analysis - How Do They Work Together?</strong></p>
<section id="goal-visualize-and-analyze-how-the-changing-proteins-interact-with-each-other-to-form-a-functional-system." class="level3">
<h3 class="anchored" data-anchor-id="goal-visualize-and-analyze-how-the-changing-proteins-interact-with-each-other-to-form-a-functional-system.">Goal: Visualize and analyze how the changing proteins interact with each other to form a functional system.</h3>
<p>Biology is not a simple list of independent proteins; it’s a complex, interconnected web of interactions. A single protein rarely acts alone. Network analysis allows us to visualize this web and find the key players or “hubs” that might be driving the biological response.</p>
</section>
<section id="the-tool-protein-protein-interaction-ppi-databases" class="level3">
<h3 class="anchored" data-anchor-id="the-tool-protein-protein-interaction-ppi-databases">The Tool: Protein-Protein Interaction (PPI) Databases</h3>
<p>Just as we needed knowledge bases for protein function, we need a database for protein interactions.</p>
<ul>
<li><p><strong>What it is:</strong> These databases collect and store evidence of how proteins physically interact with each other. This evidence can come from many sources (e-lab experiments, text mining of scientific literature, etc.).</p></li>
<li><p><strong>The Go-To Resource: STRING Database</strong></p>
<ul>
<li><p><strong>STRING (Search Tool for the Retrieval(检索) of Interacting Genes/Proteins)</strong> is arguably the most popular and comprehensive resource for this.</p></li>
<li><p>It doesn’t just store “yes/no” interactions. Each interaction in STRING has a <strong>confidence score</strong> (from 0 to 1) that represents how much evidence supports that connection. This is crucial because it allows you to filter out low-confidence or speculative interactions.</p></li>
<li><p>The evidence is color-coded by type (e.g., green for literature mining, blue for co-expression, purple for experimental evidence), so you can see why STRING thinks two proteins are connected.</p></li>
</ul></li>
</ul>
</section>
<section id="the-workflow-building-and-interpreting-a-network" class="level3">
<h3 class="anchored" data-anchor-id="the-workflow-building-and-interpreting-a-network">The Workflow: Building and Interpreting a Network</h3>
<ol type="1">
<li><p><strong>Input:</strong> You take your list of differentially abundant proteins (the same list you used for GO analysis) and paste it into the search box on the STRING website (or use a tool like Cytoscape that connects to STRING).</p></li>
<li><p><strong>Building the Network:</strong> STRING will look at your list and build a network based on its database.</p>
<ul>
<li><p><strong>Nodes (or Vertices):</strong> Each protein from your list becomes a node (a circle) in the network.</p></li>
<li><p><strong>Edges (or Links):</strong> If STRING has evidence that two proteins in your list interact, it will draw a line (an edge) between their nodes.</p></li>
</ul></li>
<li><p><strong>Initial Interpretation: Is the Network “Real”?</strong></p>
<ul>
<li><p>The first thing STRING tells you is a “PPI enrichment p-value.” This is a vital statistic. It answers the question: <strong>“Are there significantly more interactions among these proteins than you would expect for a random set of proteins of the same size?”</strong></p></li>
<li><p>If this p-value is very small (e.g., &lt; 1.0e-16), it’s a powerful result. It means your list of significant proteins is not just a random collection; they form a tightly connected biological module. This adds a huge layer of confidence to your overall findings.</p></li>
</ul></li>
<li><p><strong>Visual Analysis: Finding the Hubs and Clusters</strong></p>
<ul>
<li><p>Now you look at the network itself. You are looking for patterns:</p>
<ul>
<li><p><strong>Hubs:</strong> These are highly connected nodes (proteins with many edges). A hub protein that is significantly upregulated might be a master regulator driving the changes in its neighbors. Identifying these is often a key goal.</p></li>
<li><p><strong>Clusters (or Modules):</strong> You will often see dense “hairballs” of highly interconnected nodes within the larger network. These often represent a specific protein complex or a functional pathway. STRING can even use clustering algorithms (like k-means) to automatically color these different modules for you.</p></li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="example-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="example-interpretation">Example Interpretation</h3>
<p>Imagine you are studying a drug treatment. You create a PPI network from your upregulated proteins.</p>
<ul>
<li><p>The network has a highly significant PPI enrichment p-value.</p></li>
<li><p>You see two distinct clusters. You use your GO analysis results from the previous lesson to help interpret them.</p>
<ul>
<li><p><strong>Cluster 1:</strong> You see it’s full of proteins that your GO analysis labeled as “Proteasome components.” You’ve identified the proteasome complex, which is responsible for protein degradation.</p></li>
<li><p><strong>Cluster 2:</strong> This cluster is full of proteins related to “mRNA splicing.” You’ve identified the spliceosome.</p></li>
</ul></li>
<li><p>In the middle, connecting these two clusters, is a major <strong>hub protein</strong>, UBA52.</p></li>
</ul>
<p>This network allows you to generate a powerful hypothesis: “The drug treatment leads to the coordinated upregulation of two key cellular machines: the proteasome and the spliceosome. The central hub protein, UBA52, appears to be a key player linking these two processes. This suggests the drug may be causing cellular stress that requires both increased protein turnover and altered RNA processing.”</p>
</section>
<section id="summary-for-this-lesson-6" class="level3">
<h3 class="anchored" data-anchor-id="summary-for-this-lesson-6">Summary for this Lesson</h3>
<ul>
<li><p>Network analysis moves beyond a simple list of functions to show how your significant proteins work together as a system.</p></li>
<li><p>It relies on <strong>Protein-Protein Interaction (PPI) databases</strong>, with <strong>STRING</strong> being the most common tool.</p></li>
<li><p>The first step is to check the <strong>PPI enrichment p-value</strong> to ensure your network is statistically meaningful.</p></li>
<li><p>The main goal of visual analysis is to identify important <strong>hubs</strong> (key regulators) and <strong>clusters</strong> (functional modules), which can then be interpreted using your GO and pathway results.</p></li>
</ul>
</section>
<section id="course-conclusion" class="level3">
<h3 class="anchored" data-anchor-id="course-conclusion">Course Conclusion</h3>
<p>Congratulations! We have completed our entire journey. Starting from a complex, raw data file from the mass spectrometer, we have followed the complete bioinformatics pipeline:</p>
<ol type="1">
<li><p>We learned about the <strong>raw data structure (DDA/DIA)</strong> and performed <strong>quality control</strong>.</p></li>
<li><p>We used a <strong>search engine</strong> to match spectra to peptides from a <strong>FASTA database</strong>.</p></li>
<li><p>We used the <strong>target-decoy strategy</strong> to statistically control the <strong>False Discovery Rate</strong>.</p></li>
<li><p>We solved the <strong>protein inference problem</strong> using the principle of parsimony.</p></li>
<li><p>We <strong>quantified</strong> our proteins using either <strong>LFQ</strong> or <strong>isobaric tags</strong> and <strong>normalized</strong> the data.</p></li>
<li><p>We performed <strong>statistical tests</strong> and used a <strong>volcano plot</strong> to find the truly significant proteins.</p></li>
<li><p>Finally, we used <strong>functional enrichment</strong> and <strong>network analysis</strong> to translate that final list into a rich biological story.</p></li>
</ol>
<hr>
</section>
</section>
</section>
<section id="ture-project" class="level1">
<h1>🥺TURE Project🦄</h1>
<section id="project-the-effect-of-a-kinase-inhibitor激酶抑制剂-on-the-hela-cell-proteome" class="level3">
<h3 class="anchored" data-anchor-id="project-the-effect-of-a-kinase-inhibitor激酶抑制剂-on-the-hela-cell-proteome"><strong>Project: The Effect of a Kinase Inhibitor(激酶抑制剂) on the HeLa Cell Proteome</strong></h3>
<p><strong><em>The Biological Question:<code>"Our lab is testing a new drug, 'Kinase Inhibitor Y'. We want to know which proteins change their abundance in HeLa cancer cells after being treated with this drug for 24 hours. This will help us understand the drug's mechanism of action and potential off-target effects."</code></em></strong></p>
<p><strong>The Experimental Design:</strong></p>
<ul>
<li><p><strong>Organism:</strong> Human (Homo sapiens)</p></li>
<li><p><strong>Cell Line:</strong> HeLa (a common cancer cell line)</p></li>
<li><p><strong>Groups:</strong> Two groups, with three biological replicates each.</p>
<ul>
<li><p><strong>Control Group (3 samples):</strong> HeLa cells treated with DMSO (the solvent the drug is dissolved in).</p></li>
<li><p><strong>Treatment Group (3 samples):</strong> HeLa cells treated with Kinase Inhibitor Y.</p></li>
</ul></li>
<li><p><strong>Analysis Type:</strong> <strong><em>Label-Free Quantification (LFQ) using Data-Dependent Acquisition (DDA).</em></strong></p></li>
</ul>
</section>
<section id="part-0-project-setup-data-acquisition-and-organization" class="level3">
<h3 class="anchored" data-anchor-id="part-0-project-setup-data-acquisition-and-organization"><strong>Part 0: Project Setup, Data Acquisition, and Organization</strong></h3>
<p><strong>Goal:</strong> To download all the necessary files (raw data, protein database) and organize them logically before we even think about analysis. Good organization is 90% of the battle in bioinformatics.</p>
<section id="step-1-the-tools---your-bioinformatics-workbench" class="level4">
<h4 class="anchored" data-anchor-id="step-1-the-tools---your-bioinformatics-workbench"><strong>Step 1: The Tools - Your Bioinformatics Workbench</strong></h4>
<p>First, we need to acquire the software. For this project, we will use the most common academic standard for DDA/LFQ analysis, which is completely free.</p>
<ul>
<li><p><strong>MaxQuant:</strong> This will be our main data processing engine. It is an all-in-one program that will take our raw files, perform peak picking, search the database, control the FDR, infer proteins, and perform the label-free quantification.</p>
<ul>
<li><p><strong>Action:</strong> Go to the <a href="https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fwww.maxquant.org%2Fdownload.html"><strong>MaxQuant website</strong></a> and download the latest version. It’s a simple .zip file. Unzip it to a memorable location on your computer (e.g., C:\Bioinformatics_Tools\).</p></li>
<li><p><strong><em>By the way for me ,i rather like coding so i will use MScbase</em></strong></p></li>
</ul></li>
<li><p><strong>A Public Data Repository:</strong> Real scientific data is shared in public archives. The main repository for proteomics is <strong>PRIDE</strong>, which is part of the ProteomeXchange consortium. Our fictional dataset will have a ProteomeXchange ID.</p>
<ul>
<li><strong>Action:</strong> No software to download yet, but know that we will be getting our data from a public source like this. For very large datasets, a special fast-download client like Aspera is often used, but for our 6 files, a direct download is fine.</li>
</ul></li>
</ul>
</section>
<section id="step-2-the-data---downloading-the-raw-files" class="level4">
<h4 class="anchored" data-anchor-id="step-2-the-data---downloading-the-raw-files"><strong>Step 2: The Data - Downloading the Raw Files</strong></h4>
<p>Our lab has “deposited” the raw data to the PRIDE archive under the fictional accession number <strong>PXD012345</strong>. We need to download the 6 raw files that correspond to our experiment.</p>
<ul>
<li><p><strong>The Files:</strong></p>
<ol type="1">
<li><p><code>HeLa_Control_rep1.raw</code></p></li>
<li><p><code>HeLa_Control_rep2.raw</code></p></li>
<li><p><code>HeLa_Control_rep3.raw</code></p></li>
<li><p><code>HeLa_Inhibitor_rep1.raw</code></p></li>
<li><p><code>HeLa_Inhibitor_rep2.raw</code></p></li>
<li><p><code>HeLa_Inhibitor_rep3.raw</code></p></li>
</ol>
<p>(Note: These are Thermo .raw files, the most common type for this kind of experiment.)</p></li>
<li><p><strong>Action:</strong> You would go to the PRIDE archive, search for PXD012345, and download these 6 files.</p></li>
</ul>
</section>
<section id="step-3-project-organization---creating-a-home-for-our-analysis" class="level4">
<h4 class="anchored" data-anchor-id="step-3-project-organization---creating-a-home-for-our-analysis"><strong>Step 3: Project Organization - Creating a Home for Our Analysis</strong></h4>
<p>This is the most critical step for reproducibility and sanity. We will create a clean folder structure.</p>
<ul>
<li><p><strong>Action:</strong></p>
<ol type="1">
<li><p>Create a main project folder somewhere easy to find. Let’s call it <code>Project_HeLa_KI.</code></p></li>
<li><p>Inside <code>Project_HeLa_KI,</code> create three sub-folders:</p>
<ul>
<li><p><code>raw</code>: This is where you will move the 6 .raw files you just downloaded.</p></li>
<li><p><code>fasta</code>: This is where we will put our protein database.</p></li>
<li><p><code>analysis_output</code>: This will be the home for all the results generated by MaxQuant.</p></li>
</ul></li>
</ol></li>
</ul>
<p>Your folder structure should now look like this:</p>
<pre><code>Project_HeLa_KI/
├── raw/
│   ├── HeLa_Control_rep1.raw
│   ├── HeLa_Control_rep2.raw
│   ├── HeLa_Control_rep3.raw
│   ├── HeLa_Inhibitor_rep1.raw
│   ├── HeLa_Inhibitor_rep2.raw
│   ├── HeLa_Inhibitor_rep3.raw
├── fasta/
└── analysis_output/</code></pre>
</section>
<section id="step-4-the-protein-database---getting-the-list-of-suspects" class="level4">
<h4 class="anchored" data-anchor-id="step-4-the-protein-database---getting-the-list-of-suspects"><strong>Step 4: The Protein Database - Getting the “List of Suspects”</strong></h4>
<p>We need the correct protein database to search against. Since our samples are from human cells, we need the human proteome. The best source for this is <strong>UniProt</strong>.</p>
<ul>
<li><p><strong>Action:</strong></p>
<ol type="1">
<li><p>Go to the <a href="https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fwww.uniprot.org%2F"><strong>UniProt website</strong></a>.</p></li>
<li><p>In the search bar, search for: proteome:UP000005640 (This is the official ID for the reference human proteome).</p></li>
<li><p>On the search results page, you should see the entry for <em>Homo sapiens</em>. Click on it.</p></li>
<li><p>You will see a “Download” button. Click it. A menu will appear.</p></li>
<li><p><strong>This is important:</strong></p>
<ul>
<li><p>Under “Format”, select <strong>FASTA</strong>.</p></li>
<li><p>Under “Compressed”, select <strong>No</strong>.</p></li>
<li><p>Make sure you are downloading the <strong>Reviewed (Swiss-Prot)</strong> database. This is a smaller, higher-quality, manually curated database that is usually the best choice for standard experiments.</p></li>
</ul></li>
<li><p>Download the file. It will be named something like UP000005640_9606.fasta.</p></li>
<li><p><strong>Move this FASTA file into your fasta sub-folder.</strong></p></li>
</ol></li>
</ul>
<hr>
<p><strong><em><code>I deal with data with coding</code></em></strong></p>
</section>
</section>
<section id="part-1-r-environment-setup-data-conversion-and-project-organization" class="level3">
<h3 class="anchored" data-anchor-id="part-1-r-environment-setup-data-conversion-and-project-organization"><strong>Part 1: R Environment Setup, Data Conversion, and Project Organization</strong></h3>
<p><strong>Goal:</strong> To prepare your R environment, convert the proprietary raw data into an open format that R can read, and organize our project for a reproducible coding workflow.</p>
<section id="step-1-the-tools---your-rbioconductor-workbench" class="level4">
<h4 class="anchored" data-anchor-id="step-1-the-tools---your-rbioconductor-workbench"><strong>Step 1: The Tools - Your R/Bioconductor Workbench</strong></h4>
<p>First, we need to set up your R environment.</p>
<ol type="1">
<li><p><strong>Install R and RStudio:</strong> If you haven’t already, download and install the latest versions of <a href="https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fcran.r-project.org%2F"><strong>R</strong></a> and <a href="https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fposit.co%2Fdownload%2Frstudio-desktop%2F"><strong>RStudio Desktop</strong></a>. RStudio is a user-friendly interface for R that is highly recommended.</p></li>
<li><p><strong>Install Bioconductor:</strong> Bioconductor is a repository of R packages specifically for bioinformatics. We install its core packages using a special command. Open RStudio and run this code in the console:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">requireNamespace</span>(<span class="st">"BiocManager"</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>))     </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">"BiocManager"</span>) </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>BiocManager<span class="sc">::</span><span class="fu">install</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Install Essential Packages:</strong> Now we can install the specific packages we’ll need for this project.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>BiocManager<span class="sc">::</span><span class="fu">install</span>(<span class="fu">c</span>(<span class="st">"MSnbase"</span>, <span class="st">"ProtGenerics"</span>, <span class="st">"mzR"</span>)) </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">"tidyverse"</span>) </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For data manipulation and plotting</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><p><strong>MSnbase:</strong> This is the <strong><em>foundational package for proteomics data in R</em></strong>. It defines the data structures and provides tools for reading, manipulating, and visualizing mass spec data.</p></li>
<li><p><strong>ProtGenerics:</strong> <strong><em>Provides a common set of functions for proteomics.</em></strong></p></li>
<li><p><strong>mzR:</strong> This is the backend engine that MSnbase uses to actually read the mass spec data from the files.</p></li>
<li><p><strong>tidyverse:</strong> This is a collection of essential packages (like ggplot2 for plotting and dplyr for data wrangling) for modern data science in R.</p></li>
</ul></li>
</ol>
</section>
<section id="step-2-a-critical-prerequisite---data-conversion" class="level4">
<h4 class="anchored" data-anchor-id="step-2-a-critical-prerequisite---data-conversion"><strong>Step 2: A Critical Prerequisite - Data Conversion</strong></h4>
<p>This is the most important difference from the MaxQuant workflow. <strong><em>R packages like MSnbase cannot read the proprietary vendor formats(供应商专用格式) (e.g., Thermo’s .raw files) directly. We must first convert them to an open-standard format.</em></strong></p>
<ul>
<li><p><strong>The Open Format:</strong> <strong>mzML</strong>. This is a standardized XML-based format that all <strong><em>open-source tools can read</em></strong>.</p></li>
<li><p><strong>The Conversion Tool:</strong> <strong>ProteoWizard</strong>. This is the industry-standard, free software for converting between proteomics data formats. Its main tool is called <strong>msConvert.</strong></p></li>
<li><p><strong>Action:</strong></p>
<ol type="1">
<li><p>Go to the <a href="https://www.google.com/url?sa=E&amp;q=http%3A%2F%2Fproteowizard.sourceforge.net%2Fdownload.html"><strong>ProteoWizard website</strong></a> and download the installer for your operating system.</p></li>
<li><p>Install it.</p></li>
<li><p>Launch the MSConvertGUI application.</p></li>
<li><p>Click “Add” and select your 6 .raw files from the raw folder.</p></li>
<li><p>On the right-hand side, under “Output format”, select <strong>mzML</strong>.</p></li>
<li><p>Under “Filters”, add a filter for <strong>“Peak Picking”</strong>. Select “Vendor” as the algorithm. This performs the <strong><em>centroiding</em></strong> we discussed in our lessons, which is essential.</p></li>
<li><p>Set an output directory (you can just use the same raw folder for now) and click <strong>“Start”</strong>.</p></li>
</ol></li>
</ul>
<p>This will create 6 new files: <code>HeLa_Control_rep1.mzML, HeLa_Control_rep2.mzML, etc</code>. These are the files we will actually use in R.</p>
</section>
<section id="step-3-project-organization-the-r-way" class="level4">
<h4 class="anchored" data-anchor-id="step-3-project-organization-the-r-way"><strong>Step 3: Project Organization (The R Way)</strong></h4>
<p>A reproducible coding project needs a clean structure.</p>
<ul>
<li><p><strong>Action:</strong></p>
<ol type="1">
<li><p>Create the main project folder <code>Project_HeLa_KI_R</code>.</p></li>
<li><p>Inside it, create these sub-folders:</p>
<ul>
<li><p><code>data_raw:</code> Move your 6 new .mzML files here.</p></li>
<li><p><code>data_processed</code>: This is for processed data we save along the way.</p></li>
<li><p><code>fasta</code>: For our protein database.</p></li>
<li><p><code>scripts</code>: For our R scripts.</p></li>
<li><p><code>figures</code>: For the plots and figures we generate.</p></li>
</ul></li>
<li><p><strong>In RStudio:</strong> Go to File &gt; New Project… &gt; Existing Directory and select your <code>Project_HeLa_KI_R folder.</code> This creates an <code>.Rproj</code> file. From now on, you can just double-click this file to open RStudio, and it will automatically set your working directory to the project folder, which is incredibly helpful.</p></li>
</ol></li>
</ul>
</section>
<section id="step-4-the-protein-database-same-as-before" class="level4">
<h4 class="anchored" data-anchor-id="step-4-the-protein-database-same-as-before"><strong>Step 4: The Protein Database (Same as before)</strong></h4>
<p>This step is identical.</p>
<ul>
<li><strong>Action:</strong> Download the human reviewed (Swiss-Prot) FASTA file from UniProt (UP000005640_9606.fasta) and place it in your fasta sub-folder.</li>
</ul>
</section>
<section id="step-5-create-your-first-r-script" class="level4">
<h4 class="anchored" data-anchor-id="step-5-create-your-first-r-script"><strong>Step 5: Create Your First R Script</strong></h4>
<ul>
<li><strong>Action:</strong> In RStudio, go to File &gt; New File &gt; R Script. Save this empty script inside your scripts folder as <code>01_data_loading_and_qc.R.</code> We will write our code here.</li>
</ul>
</section>
</section>
<section id="end-of-part-1-status-check" class="level3">
<h3 class="anchored" data-anchor-id="end-of-part-1-status-check"><strong>End of Part 1: Status Check</strong></h3>
<p>Let’s review what we’ve accomplished. This is the professional setup for any R-based bioinformatics project.</p>
<ol type="1">
<li><p>We have a fully configured R environment with all the necessary specialized proteomics packages installed.</p></li>
<li><p>We have converted our proprietary raw data into the open-standard <strong>mzML format</strong> that R can read.</p></li>
<li><p>We have a perfectly organized project folder and have created an <strong>RStudio Project</strong> to manage it.</p></li>
<li><p>We have our protein database ready.</p></li>
<li><p>We have our first R script file open and ready for code.</p></li>
</ol>
<p>You are now in a position of complete control. Instead of a “black box,” every single step of the analysis from here on will be an explicit line of code that you write, which is the ultimate goal for flexibility and reproducibility.</p>
<hr>
</section>
<section id="part-2-data-loading-and-initial-quality-control-with-r" class="level3">
<h3 class="anchored" data-anchor-id="part-2-data-loading-and-initial-quality-control-with-r"><strong>Part 2: Data Loading and Initial Quality Control with R</strong></h3>
<p><strong>Goal:</strong> To load our converted <code>.mzML</code> files into R, create a single, unified experimental object, and perform initial quality control checks to <strong>ensure the data is of good quality.</strong></p>
<section id="step-1-setting-up-the-script-and-loading-libraries" class="level4">
<h4 class="anchored" data-anchor-id="step-1-setting-up-the-script-and-loading-libraries"><strong>Step 1: Setting up the Script and Loading Libraries</strong></h4>
<p>The first thing in any R script should be to load the libraries you’ll need. This makes it clear what dependencies the script has.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Part 2: Data Loading and Initial Quality Control</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MSnbase)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mzR)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># We'll use ggplot2 from this package for plotting</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(magrittr) <span class="co"># Provides the pipe operator %&gt;% for cleaner code</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a color palette for our plots</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>palette <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Control"</span> <span class="ot">=</span> <span class="st">"#1b9e77"</span>, <span class="st">"Inhibitor"</span> <span class="ot">=</span> <span class="st">"#d95f02"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Copy this code block into your R script. Highlight it and press Ctrl + Enter to run it.</li>
</ul>
</section>
<section id="step-2-locating-and-reading-the-raw-data-files" class="level4">
<h4 class="anchored" data-anchor-id="step-2-locating-and-reading-the-raw-data-files"><strong>Step 2: Locating and Reading the Raw Data Files</strong></h4>
<p>We need to tell R where our .mzML files are. We also need to create a “phenotype” data frame. This is a crucial table that describes our experimental design. It links each raw file to its experimental group.</p>
<ul>
<li></li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. Define file paths and experimental design ---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the full paths to our mzML files</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># list.files() is a great function for this</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>mzml_files <span class="ot">&lt;-</span> <span class="fu">list.files</span>(</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">path =</span> <span class="st">"./data_raw"</span>,      <span class="co"># The folder where our files are</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">pattern =</span> <span class="st">".mzML"</span>,         <span class="co"># The file extension we're looking for</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">full.names =</span> <span class="cn">TRUE</span>,       <span class="co"># Get the full path, not just the filename</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">recursive =</span> <span class="cn">FALSE</span>        <span class="co"># Don't look in sub-folders</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the result</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(mzml_files)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 'phenotype' data frame (pData) that describes the experiment</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># The row names MUST match the file names (without the path and extension)</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>pdata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_name =</span> <span class="fu">c</span>(<span class="st">"Control_1"</span>, <span class="st">"Control_2"</span>, <span class="st">"Control_3"</span>, <span class="st">"Inhibitor_1"</span>, <span class="st">"Inhibitor_2"</span>, <span class="st">"Inhibitor_3"</span>),</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Control"</span>, <span class="dv">3</span>), <span class="fu">rep</span>(<span class="st">"Inhibitor"</span>, <span class="dv">3</span>)),</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">row.names =</span> <span class="fu">basename</span>(tools<span class="sc">::</span><span class="fu">file_path_sans_ext</span>(mzml_files))</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at our experimental design table</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(pdata)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Add this code to your script and run it. You should see the list of your 6 file paths printed in the console, followed by the neatly organized pdata table. This table is the “map” for our entire experiment.</li>
</ul>
</section>
<section id="step-3-loading-the-data-into-an-msnexp-object" class="level4">
<h4 class="anchored" data-anchor-id="step-3-loading-the-data-into-an-msnexp-object"><strong>Step 3: Loading the Data into an MSnExp Object</strong></h4>
<p>This is the main event of this section. We will use the <strong><em>readMSData()</em></strong> function from the MSnbase package to load all 6 files into a single, powerful R object called an <strong><em>MSnExp</em></strong> object. This object will contain every single spectrum from every file, all neatly organized.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2. Read the raw data into an MSnExp object ---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The 'readMSData' function is the main workhorse for loading data.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># We give it our file paths and our phenotype data.</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 'mode = "onDisk"' is VERY important. It tells MSnbase to not load all the</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># spectral data into RAM, but to keep it on the hard drive. This saves memory</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># and is essential for large experiments.</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>raw_data <span class="ot">&lt;-</span> <span class="fu">readMSData</span>(</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">files =</span> mzml_files,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">pData =</span> pdata,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">msLevel. =</span> <span class="dv">1</span>, <span class="co"># We only load MS1 spectra for now for QC</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">mode =</span> <span class="st">"onDisk"</span> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's inspect the object we created</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(raw_data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. It might take a minute or two as it reads the header information from all 6 files. The output in the console will be a summary of the object, telling you how many spectra it contains from how many files. This object, <strong><em><code>raw_data</code></em></strong>, is now the center of our universe for this initial QC.</li>
</ul>
</section>
<section id="step-4-initial-quality-control---the-total-ion-chromatogram-tic总离子色谱图" class="level4">
<h4 class="anchored" data-anchor-id="step-4-initial-quality-control---the-total-ion-chromatogram-tic总离子色谱图"><strong>Step 4: Initial Quality Control - The Total Ion Chromatogram (TIC)(总离子色谱图)</strong></h4>
<p>As we discussed in our theory lessons, the first QC check is to look at the Total Ion Chromatogram (TIC). We want to see if the overall signal was stable and comparable across our runs.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. Perform Initial Quality Control ---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the chromatogram data (in this case, the TIC)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>tic_data <span class="ot">&lt;-</span> <span class="fu">chromatogram</span>(raw_data, <span class="at">aggregationFun =</span> <span class="st">"sum"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the chromatogram data to a regular data frame for plotting with ggplot2</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>tic_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(tic_data)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the structure of this data frame</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tic_df)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, create the plot using ggplot2</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>tic_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(tic_df, <span class="fu">aes</span>(<span class="at">x =</span> rtime <span class="sc">/</span> <span class="dv">60</span>, <span class="at">y =</span> intensity, <span class="at">group =</span> sample_name, <span class="at">color =</span> sample_group)) <span class="sc">+</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">lwd =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> palette) <span class="sc">+</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Total Ion Chromatogram (TIC) per Sample"</span>,</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Retention Time (minutes)"</span>,</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Intensity (sum of all ions)"</span>,</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"Experimental Group"</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"top"</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the plot</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tic_plot)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the plot to our figures folder</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"figures/01_tic_plot.png"</span>,</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>  <span class="at">plot =</span> tic_plot,</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">width =</span> <span class="dv">10</span>,</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">height =</span> <span class="dv">6</span>,</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>  <span class="at">dpi =</span> <span class="dv">300</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><p><strong>Action:</strong> Run this final block of code. In your RStudio “Plots” pane, you should see a beautiful line plot showing the 6 TICs.</p></li>
<li><p><strong>Interpretation:</strong> Look at the plot.</p>
<ul>
<li><p>Are the overall shapes roughly similar? (Good)</p></li>
<li><p>Are the peak intensities in the same general range? (Good)</p></li>
<li><p>Are there any runs where the signal is dramatically lower or where it suddenly drops to zero? (Bad - this would indicate a failed run).</p></li>
<li><p>Based on our fictional “good” data, the TICs should look consistent and well-behaved, giving us confidence to proceed.</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="end-of-part-2-status-check" class="level3">
<h3 class="anchored" data-anchor-id="end-of-part-2-status-check"><strong>End of Part 2: Status Check</strong></h3>
<p>Congratulations! You have just completed the first, crucial steps of a professional, code-based proteomics analysis.</p>
<ol type="1">
<li><p>You have successfully loaded 6 complex mzML files into a single, organized R object (raw_data).</p></li>
<li><p>You have a clean, machine-readable table (pdata) that defines your entire experimental design.</p></li>
<li><p>You have performed the most important initial QC check by generating, plotting, and saving the Total Ion Chromatograms.</p></li>
<li><p>Crucially, every single step is recorded in your R script. You could email this script to a colleague, and they could perfectly reproduce your analysis from the raw data.</p></li>
</ol>
<hr>
</section>
<section id="part-3-peptide-identification-using-a-standalone-search-engine独立搜索引擎" class="level3">
<h3 class="anchored" data-anchor-id="part-3-peptide-identification-using-a-standalone-search-engine独立搜索引擎"><strong>Part 3: Peptide Identification using a Standalone Search Engine(独立搜索引擎)</strong></h3>
<p><strong>Goal:</strong> <strong><em>To take our quality-controlled .mzML files, search them against our human protein database to identify peptides, and then perform rigorous statistical validation to generate a high-confidence list of protein identifications.</em></strong></p>
<p><strong>Our Chosen Toolchain:</strong></p>
<ul>
<li><p><strong>Search Engine:</strong> <strong>Comet</strong>. A very fast, robust, and open-source search engine.</p></li>
<li><p><strong>Statistical Validation Suite:</strong> <strong>Trans-Proteomic Pipeline (TPP)</strong>. A suite of tools that includes PeptideProphet and ProteinProphet, the <strong><em>gold standard</em></strong> for post-search statistical validation.</p></li>
</ul>
<section id="step-1-download-the-tools" class="level4">
<h4 class="anchored" data-anchor-id="step-1-download-the-tools"><strong>Step 1: Download the Tools</strong></h4>
<ul>
<li><p><strong>Action:</strong></p>
<ol type="1">
<li><p><strong>Comet:</strong> Go to the <a href="https://www.google.com/url?sa=E&amp;q=https%3A%2F%2Fcomet-ms.sourceforge.io%2F"><strong>Comet-MS website</strong></a> and download the pre-compiled binary for your operating system (e.g., Windows or Linux). Unzip it into a tools folder (e.g., C:\Bioinformatics_Tools\comet\).</p></li>
<li><p><strong>TPP:</strong> Go to the <a href="https://www.google.com/url?sa=E&amp;q=http.tools.proteomecenter.org%2Fwiki%2Findex.php%3Ftitle%3DTPP_Install"><strong>TPP website</strong></a> and download the version for your OS. The TPP is a larger suite of command-line tools. Install it to a memorable location.</p></li>
</ol></li>
</ul>
</section>
<section id="step-2-prepare-the-database-for-searching" class="level4">
<h4 class="anchored" data-anchor-id="step-2-prepare-the-database-for-searching"><strong>Step 2: Prepare the Database for Searching</strong></h4>
<p>A critical step for the Target-Decoy strategy is to create the decoy database.</p>
<ul>
<li><p><strong>Action:</strong> We will use a tool from the TPP suite to do this. Open a command prompt or terminal. Navigate to your project’s fasta directory and run the following command (you may need to provide the full path to the fasta-decoy.pl script from your TPP installation):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Command to be run in the terminal, inside the 'fasta' folder fasta-decoy.pl UP000005640_9606.fasta --decoy_prefix="decoy_" --</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="va">output_file</span><span class="op">=</span><span class="st">"human_proteome_target_decoy.fasta"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li>This command takes our original FASTA file and creates a new one called <em><code>human_proteome_target_decoy.fasta</code></em>. This new file contains all the original “target” protein sequences, plus a decoy version of each one (usually reversed) with “<em><code>decoy_</code></em>” prefixed to its name. This is the file we will give to Comet.</li>
</ul></li>
</ul>
</section>
<section id="step-3-configure-and-run-the-comet-search" class="level4">
<h4 class="anchored" data-anchor-id="step-3-configure-and-run-the-comet-search"><strong>Step 3: Configure and Run the Comet Search</strong></h4>
<p>Comet is configured using a simple text file with parameters.</p>
<ul>
<li><p><strong>Action 1: Create the parameter file.</strong></p>
<ul>
<li><p>In your main project folder (<em><code>Project_Heela_KI_R/</code></em>), create a new text file named <em><code>comet.params.txt.</code></em></p></li>
<li><p><strong><em>Copy and paste the following configuration into it</em></strong>. The comments explain each important setting.</p></li>
</ul>
<p><strong>code Ini</strong></p>
<pre><code># comet.params.txt - Configuration for our HeLa LFQ search

database_name = ./fasta/human_proteome_target_decoy.fasta
decoy_search = 0 # 0 because we are using a concatenated target-decoy database

# Mass Tolerances
peptide_mass_tolerance = 10.0
peptide_mass_units = 2  # 2 = ppm
fragment_bin_tolerance = 0.02
fragment_bin_offset = 0.0

# Modifications
variable_mod1 = 15.9949 M 0 3 -1 0 0 # Oxidation(氧化) of M
variable_mod2 = 42.0106 N-term 0 3 -1 0 0 # Acetylation(乙酰化) of protein N-terminus
add_C_carbamidomethyl = 57.02146 # This is our fixed modification on Cysteine(半胱氨酸)

# Enzyme settings
search_enzyme_number = 1  # 1 = Trypsin(胰蛋白酶)
num_enzyme_termini = 2    # 2 = fully tryptic
allowed_missed_cleavage = 2

# Output format
output_pepxml = 1 # We want pepXML output for the TPP</code></pre></li>
<li><p><strong>Action 2: Run the search from the command line.</strong></p>
<ul>
<li>This is best done with a simple loop. Open your terminal and navigate to your main project directory. Run a command like this (you’ll need to provide the full path to comet.exe):</li>
</ul></li>
<li><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This loop runs Comet on every mzML file </span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> file <span class="kw">in</span> ./data_raw/<span class="pp">*</span>.mzML<span class="kw">;</span> <span class="cf">do</span>   </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>   <span class="ex">/path/to/comet.exe</span> <span class="at">-Pcomet.params.txt</span> <span class="st">"</span><span class="va">$file</span><span class="st">"</span> </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>What this does:</strong> Comet will run on each of your <code>6 .mzML</code> files, one by one. For each input file (e.g., <code>HeLa_Control_rep1.mzML</code>), it will create an output file named <code>HeLa_Control_rep1.pep.xml.</code> This <code>.pep.xml</code> file contains the raw search results (the Peptide-Spectrum Matches). This process will take some time.</li>
</ul></li>
</ul>
</section>
<section id="step-4-perform-statistical-validation-with-the-tpp" class="level4">
<h4 class="anchored" data-anchor-id="step-4-perform-statistical-validation-with-the-tpp"><strong>Step 4: Perform Statistical Validation with the TPP</strong></h4>
<p>Now we have our 6 <code>.pep.xml</code> files full of raw scores. We need to analyze these scores statistically to calculate probabilities and control the FDR. This is what PeptideProphet and ProteinProphet are for.</p>
<ul>
<li><p><strong>Action:</strong> This is typically a two-step command-line process. In your terminal (still in the main project directory), run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Run PeptideProphet on all results to calculate peptide probabilities</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># It will analyze the scores and create new files with '.interact.pep.xml'</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="ex">xinteract</span> <span class="at">-NHeLa_Inhibitor_Combined.pep.xml</span> <span class="at">-p0.05</span> <span class="at">-l7</span> <span class="at">-PPM</span> <span class="at">-d</span><span class="st">"decoy_"</span> <span class="at">-OAPdl</span> <span class="pp">*</span>.pep.xml</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Run ProteinProphet on the combined, validated peptide evidence</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># This performs the protein inference and calculates protein-level probabilities/FDR</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="ex">proteinprophet</span> HeLa_Inhibitor_Combined.interact.pep.xml HeLa_Inhibitor_Combined.prot.xml DECOY=<span class="st">"decoy_"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><p><strong>What this does:</strong> This is a complex but standard TPP workflow.</p>
<ol type="1">
<li><p><code>xinteract</code> is a wrapper(包装器) that runs PeptideProphet. It takes all your raw <code>.pep.xml</code> files, analyzes the score distributions for target vs.&nbsp;decoy hits, and calculates a robust probability for each PSM being correct. The <code>-NHeLa_Inhibitor_Combined.pep.xml</code> option tells <code>xinteract</code> to <strong>combine the results</strong> into a single output file named <code>HeLa_Inhibitor_Combined.pep.xml</code>.</p></li>
<li><p><code>proteinprophet</code> then takes that validated peptide evidence, performs the protein inference step (using the principle of parsimony, just as we discussed), and calculates a final probability and FDR for each identified protein group.</p></li>
</ol></li>
</ul></li>
</ul>
</section>
</section>
<section id="end-of-part-3-status-check" class="level3">
<h3 class="anchored" data-anchor-id="end-of-part-3-status-check"><strong>End of Part 3: Status Check</strong></h3>
<p>This was a significant but essential detour from R. We have used a set of high-performance, specialized command-line tools to perform the most computationally intensive part of the workflow.</p>
<p><strong>The Final, Crucial Output:</strong><br>
The entire process of this part has generated one critical file: <code>HeLa_Inhibitor_Combined.prot.xml.</code></p>
<p>This single file contains:</p>
<ul>
<li><p>A list of all the protein groups identified across all 6 runs.</p></li>
<li><p>The statistical probability (and FDR) associated with each protein identification, derived from the rigorous target-decoy analysis.</p></li>
<li><p>Information about which peptides support the identification of each protein.</p></li>
</ul>
<p>We have successfully turned our raw spectra into a statistically validated and reliable list of proteins. We are now ready to bring this high-quality information back into our R environment.</p>
<hr>
</section>
<section id="part-4-importing-identifications-and-performing-ms1-level-quantification-in-r" class="level3">
<h3 class="anchored" data-anchor-id="part-4-importing-identifications-and-performing-ms1-level-quantification-in-r"><strong>Part 4: Importing Identifications and Performing MS1-Level Quantification in R</strong></h3>
<p><strong>Goal:</strong> <strong><em>To parse the identification results from our <code>prot.xml</code> file, filter them to a 1% protein FDR, and then use this high-confidence list to guide the quantification of MS1 features from the raw data we loaded in Part 2.</em></strong></p>
<p><strong>New R Packages Needed:</strong><br>
For this part, we need a way to parse(解析) the XML output from the TPP. The <strong><em><code>protViz</code></em></strong> <strong>package</strong> is excellent for this. We also need a package for quantification. We will use the <strong><em><code>aLFQ</code></em></strong> <strong>package</strong> which is specifically designed for label-free quantification workflows in R.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install new packages needed for this part </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>BiocManager<span class="sc">::</span><span class="fu">install</span>(<span class="fu">c</span>(<span class="st">"protViz"</span>, <span class="st">"aLFQ"</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Open RStudio and run the installation command above. Then, create a new R script in your scripts folder named <code>02_quantification_and_analysis.R.</code></li>
</ul>
<section id="step-1-setting-up-the-script-and-loading-data" class="level4">
<h4 class="anchored" data-anchor-id="step-1-setting-up-the-script-and-loading-data"><strong>Step 1: Setting up the Script and Loading Data</strong></h4>
<p>We’ll start our new script by loading libraries and the raw_data object (after QC) we created in Part 2.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Part 4: Identification Import and MS1 Quantification</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MSnbase)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(protViz)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(aLFQ) <span class="co"># The main package for this part</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the raw MSnExp object we created in the previous script</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># We saved it to keep our workspace clean between steps</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># (Note: In a real workflow, you would save the object at the end of Part 2</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># using save(raw_data, file = "data_processed/raw_data.RData"))</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># For this example, we will just re-run the loading from Part 2.</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Re-run from Part 2 to get the 'raw_data' object ---</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>mzml_files <span class="ot">&lt;-</span> <span class="fu">list.files</span>(<span class="st">"./data_raw"</span>, <span class="st">".mzML"</span>, <span class="at">full.names =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>pdata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_name =</span> <span class="fu">c</span>(<span class="st">"Control_1"</span>, <span class="st">"Control_2"</span>, <span class="st">"Control_3"</span>, <span class="st">"Inhibitor_1"</span>, <span class="st">"Inhibitor_2"</span>, <span class="st">"Inhibitor_3"</span>),</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample_group =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Control"</span>, <span class="dv">3</span>), <span class="fu">rep</span>(<span class="st">"Inhibitor"</span>, <span class="dv">3</span>)),</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">row.names =</span> <span class="fu">basename</span>(tools<span class="sc">::</span><span class="fu">file_path_sans_ext</span>(mzml_files))</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>raw_data <span class="ot">&lt;-</span> <span class="fu">readMSData</span>(<span class="at">files =</span> mzml_files, <span class="at">pData =</span> pdata, <span class="at">msLevel. =</span> <span class="dv">1</span>, <span class="at">mode =</span> <span class="st">"onDisk"</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Add this code to your new script. It sets up our environment and ensures the raw_data object is loaded and ready.</li>
</ul>
</section>
<section id="step-2-parse-and-filter-protein-identifications" class="level4">
<h4 class="anchored" data-anchor-id="step-2-parse-and-filter-protein-identifications"><strong>Step 2: Parse and Filter Protein Identifications</strong></h4>
<p>Now, we read the <code>prot.xml</code> file and filter it to get our high-confidence protein list.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. Parse and Filter Protein Prophet Results ---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The parse.prot.xml function from protViz reads the TPP output</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>prot_xml_path <span class="ot">&lt;-</span> <span class="st">"HeLa_Inhibitor_Combined.prot.xml"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>protein_identifications <span class="ot">&lt;-</span> <span class="fu">parse.prot.xml</span>(prot_xml_path)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's inspect the loaded data</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(protein_identifications)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter the identifications to a 1% False Discovery Rate (FDR)</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># The 'probability' column from ProteinProphet corresponds to FDR</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co"># A probability of &gt;= 0.99 corresponds to &lt;= 1% FDR</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>high_confidence_proteins <span class="ot">&lt;-</span> protein_identifications <span class="sc">%&gt;%</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(probability <span class="sc">&gt;=</span> <span class="fl">0.99</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a clean list of protein group identifiers</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>protein_list <span class="ot">&lt;-</span> <span class="fu">unique</span>(high_confidence_proteins<span class="sc">$</span>protein_group)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># How many high-confidence protein groups did we identify?</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Number of protein groups identified at &lt;1% FDR:"</span>, <span class="fu">length</span>(protein_list))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. It will read the results of our external search, apply the critical 1% FDR filter, and tell us how many reliable protein groups we’ve identified across the entire experiment. This list, <code>protein_list</code>, is our trusted set of proteins.</li>
</ul>
</section>
<section id="step-3-perform-ms1-feature-detection" class="level4">
<h4 class="anchored" data-anchor-id="step-3-perform-ms1-feature-detection"><strong>Step 3: Perform MS1 Feature Detection</strong></h4>
<p>Before we can quantify, we need to go back to our raw MS1 data and find all the “features” (the 3D peaks of m/z, retention time, and intensity). The aLFQ package has functions to do this.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 2. MS1 Feature Detection ---</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># This step finds all potential peptide peaks in the MS1 data</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># It can take a while to run on all files</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>feature_list <span class="ot">&lt;-</span> <span class="fu">findMs1Features</span>(raw_data)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's inspect the features found in the first sample</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(feature_list[[<span class="dv">1</span>]])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. This is a computationally intensive step where the algorithm scans through all the MS1 spectra to find potential peptide signals. The <code>feature_list</code> object is a list, where each element contains a table of all features found in one of our raw files.</li>
</ul>
</section>
<section id="step-4-link-identifications-to-features-and-quantify" class="level4">
<h4 class="anchored" data-anchor-id="step-4-link-identifications-to-features-and-quantify"><strong>Step 4: Link Identifications to Features and Quantify</strong></h4>
<p>This is the magic step. We now <strong><em>link our high-confidence protein identifications to the MS1 features we just found</em></strong>. This is essentially the R version of “Match Between Runs”. The aLFQ package will then calculate the area under the curve for each feature, giving us our quantification.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. Link Identifications and Quantify ---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to provide the raw search results (pep.xml) for aLFQ to link them</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>pep_xml_files <span class="ot">&lt;-</span> <span class="fu">list.files</span>(<span class="at">pattern =</span> <span class="st">".pep.xml"</span>, <span class="at">full.names =</span> <span class="cn">TRUE</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the main quantification function</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># It aligns retention times, links features to IDs, and calculates peak areas</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>protein_quant <span class="ot">&lt;-</span> <span class="fu">quantify</span>(</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">ms1.features =</span> feature_list,</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">pep.xml =</span> pep_xml_files,</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">fasta =</span> <span class="st">"./fasta/human_proteome_target_decoy.fasta"</span> <span class="co"># Used to get protein info</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a><span class="co"># The output is a tidy data frame with quantification data</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the result</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(protein_quant)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><p><strong>Action:</strong> Run this code. This is the second major computational step. The quantify function will:</p>
<ol type="1">
<li><p>Perform retention time alignment across the 6 runs.</p></li>
<li><p>Match the identified peptides (from the <code>.pep.xml</code> files) to the MS1 features based on m/z and retention time.</p></li>
<li><p>Transfer identifications to features that were not identified by MS/MS (the “match between runs” principle).</p></li>
<li><p>Carefully calculate the integrated peak area for every feature.</p></li>
<li><p>Roll up the peptide intensities to the protein level.</p></li>
</ol></li>
</ul>
<p>The resulting <code>protein_quant</code> data frame is our first look at the quantitative data.</p>
</section>
<section id="step-5-normalization-and-final-data-matrix-creation" class="level4">
<h4 class="anchored" data-anchor-id="step-5-normalization-and-final-data-matrix-creation"><strong>Step 5: Normalization and Final Data Matrix Creation</strong></h4>
<p>The raw intensities are not yet comparable. We need to normalize them. aLFQ provides a final function to do this and create our final, analysis-ready data matrix.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4. Normalization and Matrix Generation ---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># This function normalizes the data and pivots it into a wide format</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># with proteins as rows and samples as columns</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>final_lfq_matrix <span class="ot">&lt;-</span> <span class="fu">makeProteinTable</span>(protein_quant)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the final, analysis-ready matrix</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that the values are on a log2 scale, which is perfect for statistics</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(final_lfq_matrix))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this final piece of code. The makeProteinTable function performs a robust normalization (similar to median normalization) and transforms the data into the exact format we need for the next step: a matrix where each row is a protein, each column is a sample, and the values are the normalized log2-transformed intensities.</li>
</ul>
</section>
</section>
<section id="end-of-part-4-status-check" class="level3">
<h3 class="anchored" data-anchor-id="end-of-part-4-status-check"><strong>End of Part 4: Status Check</strong></h3>
<p>This was a huge and important part of the analysis. Let’s recap what we’ve built in our R script:</p>
<ol type="1">
<li><p>We have successfully imported the <strong>externally-generated, statistically validated protein identifications</strong> into R.</p></li>
<li><p>We have performed <strong>MS1 feature detection</strong> on our raw data.</p></li>
<li><p>We have <strong>linked</strong> these two sources of information to perform robust <strong>label-free quantification</strong>, including retention time alignment and “match between runs.”</p></li>
<li><p>We have <strong>normalized</strong> the resulting intensities and created a final, clean, log2-transformed <strong>data matrix</strong>.</p></li>
</ol>
<p>This <code>final_lfq_matrix</code> is the culmination of all our processing so far. It is the direct input for the final stage of the project: differential expression analysis and biological interpretation.</p>
<hr>
</section>
<section id="filter-the-protein" class="level3">
<h3 class="anchored" data-anchor-id="filter-the-protein">Filter the Protein</h3>
<p><strong><em>You might find out a detail that we did not directly use the protein_list to filter the protein before we quantify the protein,it seems could help us save memory but the truth is not</em></strong></p>
<p>The short answer is: <strong>the quantify function in aLFQ does not directly use our filtered protein_list. Instead, it wisely performs its own internal filtering and linking at the peptide level.</strong><br>
</p>
</section>
<section id="the-problem-with-using-a-pre-filtered-protein-list-for-quantification" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-with-using-a-pre-filtered-protein-list-for-quantification">The Problem with Using a Pre-Filtered Protein List for Quantification</h3>
<p>Imagine we did it the simple way: we give the quantify function our high-confidence protein_list and tell it, “Only quantify the peptides that belong to these proteins.”</p>
<p>This would lead to several problems:</p>
<ol type="1">
<li><p><strong>Loss of Peptide-Level Information:</strong> The 1% FDR for proteins was calculated by ProteinProphet based on the combined evidence of all peptides. A protein might be identified with high confidence (e.g., it has 10 supporting peptides), <strong><em>but one of those 10 peptides might be a low-confidence or ambiguous identification on its own.</em></strong> If we filter at the protein level first, we can’t make these granular(精细), peptide-level decisions during quantification.</p></li>
<li><p><strong>Incorrect Quantification of Shared Peptides:</strong> This is the <strong><em>most critical issue</em></strong>. Remember the protein inference problem? A single peptide can map to multiple proteins (e.g., Peptide P maps to Protein A and Protein B). The aLFQ quantification algorithm needs to know this. <strong><em>It needs the full, unfiltered peptide evidence to intelligently decide how to “distribute” the intensity of that shared peptide.</em></strong> If we only give it a final protein list, it loses all this crucial connectivity information.</p></li>
<li><p><strong>Sub-optimal “Match Between Runs”(次优的运行间匹配):</strong> The “match between runs” or feature transfer works best when it has a comprehensive library of all plausibly identified peptides, their m/z values, and their retention times. <strong><em>If we pre-filter the protein list, we might discard a protein that was identified with 2% FDR.</em></strong> However, a peptide from that protein might be clearly visible and quantifiable in the MS1 data. By using the full peptide evidence from the <code>.pep.xml</code> files, the quantify function has a much larger and more sensitive library to use for transferring identifications.</p></li>
</ol>
</section>
<section id="the-smarter-approach-used-by-alfq-and-maxquant" class="level3">
<h3 class="anchored" data-anchor-id="the-smarter-approach-used-by-alfq-and-maxquant">The Smarter Approach Used by aLFQ (and MaxQuant)</h3>
<p>The quantify function in aLFQ takes a more sophisticated, bottom-up approach, which is why we give it the <strong>raw .pep.xml search results</strong>, not our filtered protein list.</p>
<p>Here is its internal logic:</p>
<ol type="1">
<li><p><strong>Start with Peptides:</strong> It first looks at the <strong>Peptide-Spectrum Matches (PSMs)</strong> from the .pep.xml files. It will apply its own internal quality filter here (e.g., using the PeptideProphet probability for each individual PSM).</p></li>
<li><p><strong>Match PSMs to MS1 Features:</strong> It creates a library of high-quality identified peptides with their measured m/z and retention times. It then uses this library to find and link the MS1 features we detected in Part 4, Step 3. This is where the alignment and “match between runs” happens.</p></li>
<li><p><strong>Quantify Peptides:</strong> It calculates the peak area for all these linked MS1 features, resulting in a quantified list of peptides.</p></li>
<li><p><strong>Perform Protein Inference and Roll-up:</strong> <strong>Only now</strong>, at the very end, does it deal with the protein problem. It uses the peptide-to-protein mapping information (which is also in the .pep.xml and FASTA files) to:</p>
<ul>
<li><p>Resolve shared peptides.</p></li>
<li><p>Sum up the intensities of unique and shared peptides to calculate a final abundance for each <strong>protein group</strong>.</p></li>
<li><p>It effectively re-does the protein inference step in a quantification-aware manner.</p></li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Part 5: Differential Expression Analysis (CORRECTED WORKFLOW)</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We start with 'final_lfq_matrix' from the end of Part 4.</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># It contains the quantified data for many proteins.</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># --- NEW STEP: Filter Quantification Matrix with High-Confidence Protein List ---</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the step where we explicitly use our 'protein_list'</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's get the protein IDs from our quantified matrix</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>quantified_proteins <span class="ot">&lt;-</span> <span class="fu">rownames</span>(final_lfq_matrix)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># We must clean these IDs to match the format in 'protein_list'</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co"># (i.e., remove isoform suffixes like -1, -2 etc.)</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>quantified_proteins_clean <span class="ot">&lt;-</span> <span class="fu">str_remove</span>(quantified_proteins, <span class="st">"-</span><span class="sc">\\</span><span class="st">d+"</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, we perform the crucial filtering operation.</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co"># We check which of our quantified proteins are present in our high-confidence list.</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>is_high_confidence <span class="ot">&lt;-</span> quantified_proteins_clean <span class="sc">%in%</span> protein_list</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the new, validated matrix by keeping only the rows that correspond</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a><span class="co"># to our high-confidence proteins.</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>matrix_validated <span class="ot">&lt;-</span> final_lfq_matrix[is_high_confidence, ]</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's print a summary to see the effect of this filtering</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Number of proteins in original quantification matrix:"</span>, <span class="fu">nrow</span>(final_lfq_matrix), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Number of proteins in our high-confidence list:"</span>, <span class="fu">length</span>(protein_list), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Number of proteins in the final validated matrix for analysis:"</span>, <span class="fu">nrow</span>(matrix_validated), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="co"># --- From this point on, we use 'matrix_validated' for all downstream steps ---</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Handling Missing Values (Now on the validated matrix)</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Note the change from 'final_lfq_matrix' to 'matrix_validated'</span></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="fu">colSums</span>(<span class="fu">is.na</span>(matrix_validated))</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>keep <span class="ot">&lt;-</span> <span class="fu">rowSums</span>(<span class="sc">!</span><span class="fu">is.na</span>(matrix_validated)) <span class="sc">&gt;=</span> <span class="dv">3</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>matrix_filtered <span class="ot">&lt;-</span> matrix_validated[keep, ]</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="co"># ... and the rest of the limma analysis would proceed exactly as before,</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="co"># but starting with 'matrix_filtered' which is derived from our validated matrix.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
</ol>
<hr>
</section>
<section id="part-5-differential-expression-analysis-and-biological-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="part-5-differential-expression-analysis-and-biological-interpretation"><strong>Part 5: Differential Expression Analysis and Biological Interpretation</strong></h3>
<p><strong>Goal:</strong> <strong><em>To take our final quantified protein matrix, fit a statistical model to identify proteins that are significantly up- or down-regulated, visualize these results with a volcano plot, and prepare the list of “hits” for downstream functional analysis.</em></strong></p>
<p><strong>New R Package Needed:</strong><br>
For this part, we will use the <em><code>limma</code></em> package. It is the gold standard in the R/Bioconductor ecosystem for analyzing differential expression, originally built for microarrays but perfectly adapted for proteomics and other ’omics data. Its use of linear models and empirical Bayes statistics makes it more powerful and reliable than running thousands of simple t-tests.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install the limma package </span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>BiocManager<span class="sc">::</span><span class="fu">install</span>(<span class="st">"limma"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Install <em><code>limma.</code></em> We will continue writing in our <em><code>02_quantification_and_analysis.R</code></em> script.</li>
</ul>
<section id="step-1-setting-up-the-script-continued" class="level4">
<h4 class="anchored" data-anchor-id="step-1-setting-up-the-script-continued"><strong>Step 1: Setting up the Script (Continued)</strong></h4>
<p>We’ll start by loading the new library at the top of our script.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># At the top of 02_quantification_and_analysis.R, add limma</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(limma)<span class="st">```</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="at">*   **Action:** Add </span><span class="st">`</span>limma<span class="st">`</span><span class="at"> to your library loading section. For the rest of this part, we will be appending code to the end of the script, working with the </span><span class="st">`</span>final_lfq_matrix<span class="st">`</span><span class="at"> we created in Part 4.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="at">#### **Step 2: Handling Missing Values**</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="at">The </span><span class="st">`</span>final_lfq_matrix<span class="st">`</span><span class="at"> may still have some </span><span class="st">`</span>NA<span class="st">`</span><span class="at"> (missing) values, especially for low-abundance proteins. </span><span class="st">`</span>limma<span class="st">`</span><span class="at"> cannot work with </span><span class="st">`</span>NA<span class="st">`</span><span class="at">s, so we must address them. A common strategy is to decide on a threshold: if a protein is missing in too many samples, we remove it; otherwise, we can impute the missing values.</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="at"># --- 5. Differential Expression Analysis with limma ---</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="at"># First, let's inspect the extent of missing data</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="at"># Count the NAs in each column (sample)</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="at">colSums(is.na(final_lfq_matrix))</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="at"># A common filtering strategy: keep proteins that are present in at least</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="at"># 2 out of 3 replicates in at least one experimental group.</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="at"># For simplicity in this example, we will keep proteins that are present</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="at"># in at least 3 of our 6 total samples.</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="at">keep &lt;- rowSums(!is.na(final_lfq_matrix)) &gt;= 3</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="at">matrix_filtered &lt;- final_lfq_matrix[keep, ]</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="at"># For the remaining NAs, we need to impute. Since these are likely</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="at"># low-abundance proteins (missing not at random), we will impute with</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="at"># a low value from the tail of the data distribution.</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="at"># NOTE: Imputation is a complex topic. This is a simple but common approach.</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="at"># For robust imputation, dedicated packages like </span><span class="st">`</span>imputeLCMD<span class="st">`</span><span class="at"> are excellent.</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="at"># Simple imputation: replace NAs with a value slightly lower than the minimum</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="at">min_val &lt;- min(matrix_filtered, na.rm = TRUE)</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="at">matrix_imputed &lt;- replace(</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="at">  matrix_filtered,</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="at">  is.na(matrix_filtered),</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="at">  min_val - runif(sum(is.na(matrix_filtered)), 0, 1) # Add jitter to avoid identical values</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="at">)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. We first filter out proteins with too many missing values and then replace the <strong><em>remaining NAs with a small, plausible value</em></strong>. Our data matrix <code>matrix_imputed</code> is now complete and ready for modeling.</li>
</ul>
</section>
<section id="step-3-setting-up-the-linear-model" class="level4">
<h4 class="anchored" data-anchor-id="step-3-setting-up-the-linear-model"><strong>Step 3: Setting up the Linear Model</strong></h4>
<p>This is the core of <em><code>limma.</code></em> We need to create two matrices:</p>
<ol type="1">
<li><p><strong>Design Matrix:</strong> This tells <em><code>limma</code></em> which sample belongs to which group.</p></li>
<li><p><strong>Contrast Matrix:</strong> This tells <em><code>limma</code></em> which comparison we want to make (in our case, Inhibitor vs.&nbsp;Control).</p></li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the design matrix</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># It models the data based on our sample_group factor from pdata</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>design <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span> <span class="dv">0</span> <span class="sc">+</span> pdata<span class="sc">$</span>sample_group)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(design) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Control"</span>, <span class="st">"Inhibitor"</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(design) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(matrix_imputed)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the design matrix to understand it</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(design)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the contrast matrix</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># This defines the comparison we are interested in</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>contrast_matrix <span class="ot">&lt;-</span> <span class="fu">makeContrasts</span>(</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">Inhibitor_vs_Control =</span> Inhibitor <span class="sc">-</span> Control,</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">levels =</span> design</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the contrast matrix</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(contrast_matrix)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. The <code>design</code> matrix is a simple binary matrix indicating group membership. The <code>contrast</code> matrix specifies the exact subtraction we want to perform to get our fold change.</li>
</ul>
</section>
<section id="step-4-fitting-the-model-and-extracting-results" class="level4">
<h4 class="anchored" data-anchor-id="step-4-fitting-the-model-and-extracting-results"><strong>Step 4: Fitting the Model and Extracting Results</strong></h4>
<p>Now we run the three main <code>limma</code> functions: <code>lmFit, contrasts.fit, and eBayes.</code></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Fit the linear model for each protein</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lmFit</span>(matrix_imputed, design)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Apply the contrast (perform the comparison)</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">contrasts.fit</span>(fit, contrast_matrix)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Apply Empirical Bayes smoothing</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the "magic" of limma. It borrows information across all proteins</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># to get more stable and reliable variance estimates, increasing our statistical power.</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">eBayes</span>(fit2)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, extract a final, tidy results table</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co"># We ask for all proteins (n = Inf)</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>results_table <span class="ot">&lt;-</span> <span class="fu">topTable</span>(fit2, <span class="at">number =</span> <span class="cn">Inf</span>, <span class="at">sort.by =</span> <span class="st">"P"</span>)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the top results</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(results_table)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><p><strong>Action:</strong> Run this code. The r<code>esults_table</code> is our final, rich output. For every protein, it contains:</p>
<ul>
<li><p>logFC: The log2 Fold Change (the effect size).</p></li>
<li><p>P.Value: The raw p-value.</p></li>
<li><p>adj.P.Val: The <strong>adjusted p-value</strong> (or q-value), corrected for multiple testing using the <code>Benjamini-Hochberg method</code>. This is the most important value for significance.</p></li>
<li><p>t: The t-statistic.</p></li>
<li><p>B: The B-statistic (log-odds that the protein is differentially expressed).</p></li>
</ul></li>
</ul>
</section>
<section id="step-5-visualizing-results-with-a-volcano-plot" class="level4">
<h4 class="anchored" data-anchor-id="step-5-visualizing-results-with-a-volcano-plot"><strong>Step 5: Visualizing Results with a Volcano Plot</strong></h4>
<p>The best way to visualize these results is with a volcano plot.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 6. Visualization and Interpretation ---</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new column to label significant proteins</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>results_table <span class="ot">&lt;-</span> results_table <span class="sc">%&gt;%</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">significance =</span> <span class="fu">case_when</span>(</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>      logFC <span class="sc">&gt;</span> <span class="dv">1</span> <span class="sc">&amp;</span> adj.P.Val <span class="sc">&lt;</span> <span class="fl">0.05</span> <span class="sc">~</span> <span class="st">"Upregulated"</span>,</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>      logFC <span class="sc">&lt;</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">&amp;</span> adj.P.Val <span class="sc">&lt;</span> <span class="fl">0.05</span> <span class="sc">~</span> <span class="st">"Downregulated"</span>,</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="st">"Not Significant"</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the volcano plot using ggplot2</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>volcano_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(results_table, <span class="fu">aes</span>(<span class="at">x =</span> logFC, <span class="at">y =</span> <span class="sc">-</span><span class="fu">log10</span>(adj.P.Val), <span class="at">color =</span> significance)) <span class="sc">+</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">size =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"Upregulated"</span> <span class="ot">=</span> <span class="st">"#d95f02"</span>, <span class="st">"Downregulated"</span> <span class="ot">=</span> <span class="st">"#1b9e77"</span>, <span class="st">"Not Significant"</span> <span class="ot">=</span> <span class="st">"grey"</span>)) <span class="sc">+</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Inhibitor vs. Control Treatment"</span>,</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">"Differentially Abundant Proteins"</span>,</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"log2(Fold Change)"</span>,</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"-log10(Adjusted p-value)"</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Add threshold lines</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="sc">-</span><span class="fu">log10</span>(<span class="fl">0.05</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the plot</span></span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(volcano_plot)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the plot</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="st">"figures/02_volcano_plot.png"</span>, volcano_plot, <span class="at">width =</span> <span class="dv">8</span>, <span class="at">height =</span> <span class="dv">7</span>, <span class="at">dpi =</span> <span class="dv">300</span>)</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Finally, get our list of significant "hits" for the next stage (functional analysis)</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>significant_hits <span class="ot">&lt;-</span> results_table <span class="sc">%&gt;%</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(significance <span class="sc">!=</span> <span class="st">"Not Significant"</span>)</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Number of significantly changed proteins:"</span>, <span class="fu">nrow</span>(significant_hits))</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(significant_hits))</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a><span class="st">```</span><span class="at">*   **Action:** Run this final block. This will generate and save your volcano plot and create a final data frame, </span><span class="st">`</span>significant_hits<span class="st">`</span><span class="at">, containing only the proteins that passed our thresholds (e.g., &gt;2-fold change and &lt;5% FDR).</span></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a><span class="at">---</span></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a><span class="at">### **End of Project: Final Status and Next Steps**</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a><span class="at">**Congratulations! You have completed a full, end-to-end, code-based proteomics analysis.**</span></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a><span class="at">Let's recap the entire journey:</span></span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a><span class="at">1.  **Part 1:** We set up our R environment and converted raw data to an open format.</span></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a><span class="at">2.  **Part 2:** We loaded the data into R and performed initial QC checks (TIC plot).</span></span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a><span class="at">3.  **Part 3:** We used powerful external tools (Comet, TPP) to perform database searching and rigorous statistical validation (FDR control).</span></span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a><span class="at">4.  **Part 4:** We brought the identification results back into R and performed MS1-level label-free quantification, alignment, and normalization.</span></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a><span class="at">5.  **Part 5:** We used the state-of-the-art </span><span class="st">`</span>limma<span class="st">`</span><span class="at"> package to perform differential expression analysis and visualized the results in a volcano plot.</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a><span class="at">**The Final Output:**</span></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a><span class="at">You now have the </span><span class="st">`</span>significant_hits<span class="st">`</span><span class="at"> data frame. This list of protein IDs, along with their fold changes and p-values, is the direct input for the biological interpretation steps we discussed in our theory lessons: **Functional Enrichment Analysis (GO, KEGG)** and **Network Analysis (STRING)**. You would now use R packages like </span><span class="st">`</span>clusterProfiler<span class="st">`</span><span class="at"> or </span><span class="st">`</span>STRINGdb<span class="st">`</span><span class="at"> to discover the biological story hidden in your list of hits.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
</section>
</section>
<section id="part-6-biological-interpretation---uncovering-the-story" class="level3">
<h3 class="anchored" data-anchor-id="part-6-biological-interpretation---uncovering-the-story"><strong>Part 6: Biological Interpretation - Uncovering the Story</strong></h3>
<p><strong>Goal:</strong> To take our list of differentially abundant proteins and use functional enrichment and network analysis to understand the underlying biological processes, pathways, and interactions that are affected by the kinase inhibitor.</p>
<p><strong>New R Packages Needed:</strong></p>
<ul>
<li><p><strong>clusterProfiler:</strong> The gold standard Bioconductor package for functional enrichment analysis (GO, KEGG, and more).</p></li>
<li><p><strong>org.Hs.eg.db:</strong> The organism-specific annotation package for Homo sapiens. This package contains the mappings between different types of gene/protein identifiers.</p></li>
<li><p><strong>STRINGdb:</strong> An R client for the STRING database to perform PPI network analysis.</p></li>
<li><p><strong>AnnotationDbi:</strong> A core Bioconductor package for working with annotation databases.</p></li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Install new packages needed for this part </span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>BiocManager<span class="sc">::</span><span class="fu">install</span>(<span class="fu">c</span>(<span class="st">"clusterProfiler"</span>, <span class="st">"org.Hs.eg.db"</span>, <span class="st">"STRINGdb"</span>, <span class="st">"AnnotationDbi"</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Install these packages. We will create a new, final R script in our scripts folder named <code>03_interpretation.R.</code></li>
</ul>
<section id="step-1-setting-up-the-script-and-loading-results" class="level4">
<h4 class="anchored" data-anchor-id="step-1-setting-up-the-script-and-loading-results"><strong>Step 1: Setting up the Script and Loading Results</strong></h4>
<p>We start by loading our libraries and the results from the previous analysis.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Part 6: Biological Interpretation and Functional Analysis</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># --------------------------------------------------------------------------</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(clusterProfiler)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(org.Hs.eg.db) <span class="co"># Human-specific annotations</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(STRINGdb)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(AnnotationDbi)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co"># For a real project, you would save your results from the previous script and load them here</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co"># At the end of script 02, you would run:</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co"># save(results_table, file = "data_processed/limma_results.RData")</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co"># In this script, you would start by loading:</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="co"># load("data_processed/limma_results.RData")</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co"># For this continuous example, we will just assume 'results_table' is in our environment</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's re-create our 'significant_hits' list</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>significant_hits <span class="ot">&lt;-</span> results_table <span class="sc">%&gt;%</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(adj.P.Val <span class="sc">&lt;</span> <span class="fl">0.05</span> <span class="sc">&amp;</span> <span class="fu">abs</span>(logFC) <span class="sc">&gt;</span> <span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Add this code to your new script. It sets up our environment and defines our list of significant proteins to work with.</li>
</ul>
</section>
<section id="step-2-a-crucial-step---id-mapping" class="level4">
<h4 class="anchored" data-anchor-id="step-2-a-crucial-step---id-mapping"><strong>Step 2: A Crucial Step - ID Mapping</strong></h4>
<p>The protein IDs in our results are likely UniProt IDs (e.g., “P04637”). Most enrichment tools, like clusterProfiler, <strong><em>work best with Entrez Gene IDs (e.g., “7157”). Our first job is to map our IDs.</em></strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 1. Map UniProt IDs to Entrez Gene IDs ---</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The row names of our results are the protein IDs</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>uniprot_ids <span class="ot">&lt;-</span> <span class="fu">rownames</span>(significant_hits)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The 'mapIds' function is the tool for this job</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to clean up potential isoform IDs first (e.g., P04637-2)</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>uniprot_ids_clean <span class="ot">&lt;-</span> <span class="fu">str_remove</span>(uniprot_ids, <span class="st">"-</span><span class="sc">\\</span><span class="st">d+"</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>entrez_ids <span class="ot">&lt;-</span> <span class="fu">mapIds</span>(</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>  org.Hs.eg.db,</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">keys =</span> uniprot_ids_clean,</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">keytype =</span> <span class="st">"UNIPROT"</span>,</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">column =</span> <span class="st">"ENTREZID"</span>,</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">multiVals =</span> <span class="st">"first"</span> <span class="co"># If one UniProt ID maps to multiple Entrez IDs, take the first one</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the Entrez IDs to our results table</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>significant_hits<span class="sc">$</span>entrez <span class="ot">&lt;-</span> entrez_ids</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove any proteins for which we couldn't find an Entrez ID</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>significant_hits_mapped <span class="ot">&lt;-</span> significant_hits <span class="sc">%&gt;%</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(entrez))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. You now have a results table with a new column for Entrez IDs, ready for <code>clusterProfiler</code>.</li>
</ul>
</section>
<section id="step-3-gene-ontology-go-enrichment-analysis" class="level4">
<h4 class="anchored" data-anchor-id="step-3-gene-ontology-go-enrichment-analysis"><strong>Step 3: Gene Ontology (GO) Enrichment Analysis</strong></h4>
<p>Let’s find out which “Biological Processes” are over-represented in our list of significant proteins.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="do">## --- 2. Gene Ontology (GO) Enrichment Analysis ---</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We need a "universe" - all the proteins we identified, not just the significant ones.</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># This is crucial for a correct statistical test.</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>all_protein_ids <span class="ot">&lt;-</span> <span class="fu">str_remove</span>(<span class="fu">rownames</span>(results_table), <span class="st">"-</span><span class="sc">\\</span><span class="st">d+"</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>universe_entrez <span class="ot">&lt;-</span> <span class="fu">mapIds</span>(org.Hs.eg.db, <span class="at">keys =</span> all_protein_ids, <span class="at">keytype =</span> <span class="st">"UNIPROT"</span>, <span class="at">column =</span> <span class="st">"ENTREZID"</span>, <span class="at">multiVals =</span> <span class="st">"first"</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>universe_entrez_clean <span class="ot">&lt;-</span> <span class="fu">unique</span>(universe_entrez[<span class="sc">!</span><span class="fu">is.na</span>(universe_entrez)])</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the enrichment analysis</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>go_results <span class="ot">&lt;-</span> <span class="fu">enrichGO</span>(</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">gene =</span> significant_hits_mapped<span class="sc">$</span>entrez,</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">universe =</span> universe_entrez_clean,</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">OrgDb =</span> org.Hs.eg.db,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">keyType =</span> <span class="st">"ENTREZID"</span>,</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">ont =</span> <span class="st">"BP"</span>, <span class="co"># BP = Biological Process, can also be "MF" or "CC"</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">pAdjustMethod =</span> <span class="st">"BH"</span>,</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">pvalueCutoff =</span> <span class="fl">0.05</span>,</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">qvalueCutoff =</span> <span class="fl">0.1</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's look at the results</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">as.data.frame</span>(go_results))</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="co"># The best way to visualize is with a dot plot</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="fu">dotplot</span>(go_results, <span class="at">showCategory =</span> <span class="dv">15</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"Enriched GO Biological Processes"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. It performs the hypergeometric test for all GO terms. The resulting table and dot plot will show you the most significantly affected biological processes. You might see terms like “cell cycle regulation,” “response to DNA damage,” or “apoptosis,” giving you your first major clue about the drug’s effect.</li>
</ul>
</section>
<section id="step-4-kegg-pathway-enrichment-analysis" class="level4">
<h4 class="anchored" data-anchor-id="step-4-kegg-pathway-enrichment-analysis"><strong>Step 4: KEGG Pathway Enrichment Analysis</strong></h4>
<p>Now let’s see if our proteins map to any known molecular pathways.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 3. KEGG Pathway Enrichment Analysis ---</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the enrichment analysis for KEGG</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>kegg_results <span class="ot">&lt;-</span> <span class="fu">enrichKEGG</span>(</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">gene =</span> significant_hits_mapped<span class="sc">$</span>entrez,</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">universe =</span> universe_entrez_clean,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">organism =</span> <span class="st">"hsa"</span>, <span class="co"># hsa = Homo sapiens</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">pvalueCutoff =</span> <span class="fl">0.05</span>,</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">qvalueCutoff =</span> <span class="fl">0.1</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the results</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="fu">dotplot</span>(kegg_results, <span class="at">showCategory =</span> <span class="dv">15</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"Enriched KEGG Pathways"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. This is a very powerful step. If the dot plot shows an enrichment for the “p53 signaling pathway,” for example, you have a very strong hypothesis that your kinase inhibitor interacts with this famous cancer-related pathway.</li>
</ul>
</section>
<section id="step-5-protein-protein-interaction-ppi-network-analysis" class="level4">
<h4 class="anchored" data-anchor-id="step-5-protein-protein-interaction-ppi-network-analysis"><strong>Step 5: Protein-Protein Interaction (PPI) Network Analysis</strong></h4>
<p>Finally, let’s see how our significant proteins interact with each other.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- 4. Protein-Protein Interaction (PPI) Network ---</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the STRING database object</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>string_db <span class="ot">&lt;-</span> STRINGdb<span class="sc">$</span><span class="fu">new</span>(<span class="at">version =</span> <span class="st">"11.5"</span>, <span class="at">species =</span> <span class="dv">9606</span>, <span class="co"># 9606 is the species code for Human</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">score_threshold =</span> <span class="dv">400</span>, <span class="at">input_directory =</span> <span class="st">""</span>) <span class="co"># 400 is a medium confidence score</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Map our protein IDs to STRING IDs</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>mapped_proteins <span class="ot">&lt;-</span> string_db<span class="sc">$</span><span class="fu">map</span>(significant_hits_mapped, <span class="st">"entrez"</span>, <span class="at">removeUnmappedRows =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the interactions for our mapped proteins</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>interactions <span class="ot">&lt;-</span> string_db<span class="sc">$</span><span class="fu">get_interactions</span>(mapped_proteins<span class="sc">$</span>STRING_id)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the network</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co"># We can color the nodes by their up/down regulation</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a payload for coloring</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>payload <span class="ot">&lt;-</span> mapped_proteins <span class="sc">%&gt;%</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">color =</span> <span class="fu">ifelse</span>(logFC <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">"#d95f02"</span>, <span class="st">"#1b9e77"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(STRING_id, color)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>string_db<span class="sc">$</span><span class="fu">plot_network</span>(mapped_proteins<span class="sc">$</span>STRING_id, <span class="at">payload =</span> payload)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<ul>
<li><strong>Action:</strong> Run this code. This will generate an image of your interaction network. Look for clusters of nodes (modules) and highly connected nodes (hubs). For example, you might see that TP53 (the protein name for p53) is a central hub connected to many other downregulated proteins, visually confirming the result from your KEGG analysis.</li>
</ul>
</section>
</section>
<section id="grand-conclusion-from-raw-data-to-a-biological-hypothesis" class="level3">
<h3 class="anchored" data-anchor-id="grand-conclusion-from-raw-data-to-a-biological-hypothesis"><strong>Grand Conclusion: From Raw Data to a Biological Hypothesis</strong></h3>
<p>We have completed the entire journey. Let’s synthesize our fictional results into a coherent story that you could present in a lab meeting.</p>
<ol type="1">
<li><p><strong>Project Setup:</strong> We started with 6 raw mass spec files from a well-defined experiment comparing a kinase inhibitor to a control.</p></li>
<li><p><strong>Processing &amp; QC:</strong> We used R to load the data, confirmed its quality, and then used a high-performance external search engine (Comet/TPP) to get a list of <strong>~3,500 high-confidence protein groups</strong> at a 1% FDR.</p></li>
<li><p><strong>Quantification:</strong> We brought these identifications back into R and performed robust label-free quantification, resulting in a clean, normalized data matrix.</p></li>
<li><p><strong>Statistical Analysis:</strong> Using limma, we ran a differential expression analysis and found <strong>150 proteins</strong> that were significantly up- or down-regulated (our significant_hits).</p></li>
<li><p><strong>Biological Interpretation:</strong> Our functional analysis of these 150 hits revealed a powerful story:</p>
<ul>
<li><p><strong>GO Analysis:</strong> Showed a strong enrichment for terms like “regulation of cell cycle” and “apoptotic process.”</p></li>
<li><p><strong>KEGG Analysis:</strong> Pinpointed the “p53 signaling pathway” as the most significantly affected pathway.</p></li>
<li><p><strong>PPI Network Analysis:</strong> Visualized these proteins, revealing a tightly interconnected module of downregulated proteins centered on the famous tumor suppressor <strong>TP53</strong> as a major hub.</p></li>
</ul></li>
</ol>
<p><strong>The Final Hypothesis:</strong><br>
“Our proteomic analysis demonstrates that Kinase Inhibitor Y has a significant impact on the HeLa cell proteome. The drug causes a coordinated downregulation of key proteins involved in the p53 signaling pathway. The central tumor suppressor protein, TP53, appears to be a key hub in this response, which is linked to an observed enrichment in proteins that regulate the cell cycle and apoptosis. This strongly suggests the drug’s mechanism of action involves the activation of p53-mediated tumor suppressive functions.”</p>
</section>
</section>
     </main>
<!-- /main column -->  <script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>  </div> <!-- /content --> 
  
</body></html>